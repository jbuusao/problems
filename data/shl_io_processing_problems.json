def calculate_median(sorted_data):\n    \"\"\"Calculate median from sorted data\"\"\"\n    n = len(sorted_data)\n    if n % 2 == 0:\n        return (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n    else:\n        return sorted_data[n//2]\n\ndef calculate_mode(data):\n    \"\"\"Calculate mode(s) from data\"\"\"\n    if not data:\n        return None\n    \n    frequency = Counter(data)\n    max_freq = max(frequency.values())\n    modes = [value for value, freq in frequency.items() if freq == max_freq]\n    \n    return {\n        'values': modes,\n        'frequency': max_freq,\n        'is_multimodal': len(modes) > 1\n    }\n\ndef calculate_percentile(sorted_data, percentile):\n    \"\"\"Calculate percentile from sorted data\"\"\"\n    if not sorted_data:\n        return 0\n    \n    n = len(sorted_data)\n    if percentile == 0:\n        return sorted_data[0]\n    if percentile == 100:\n        return sorted_data[-1]\n    \n    index = (percentile / 100) * (n - 1)\n    lower_index = int(index)\n    upper_index = min(lower_index + 1, n - 1)\n    \n    if lower_index == upper_index:\n        return sorted_data[lower_index]\n    \n    # Linear interpolation\n    weight = index - lower_index\n    return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight\n\ndef detect_outliers(data):\n    \"\"\"Detect outliers using IQR method\"\"\"\n    if len(data) < 4:\n        return []\n    \n    sorted_data = sorted(data)\n    q1 = calculate_percentile(sorted_data, 25)\n    q3 = calculate_percentile(sorted_data, 75)\n    iqr = q3 - q1\n    \n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    outliers = []\n    for value in data:\n        if value < lower_bound or value > upper_bound:\n            outliers.append({\n                'value': value,\n                'type': 'low' if value < lower_bound else 'high',\n                'distance_from_bound': abs(value - (lower_bound if value < lower_bound else upper_bound))\n            })\n    \n    return outliers\n\ndef calculate_skewness(data, mean, std_dev):\n    \"\"\"Calculate skewness (measure of asymmetry)\"\"\"\n    if std_dev == 0 or len(data) < 3:\n        return 0\n    \n    n = len(data)\n    skew = sum(((x - mean) / std_dev) ** 3 for x in data) / n\n    return round(skew, 6)\n\ndef calculate_kurtosis(data, mean, std_dev):\n    \"\"\"Calculate kurtosis (measure of tail heaviness)\"\"\"\n    if std_dev == 0 or len(data) < 4:\n        return 0\n    \n    n = len(data)\n    kurt = sum(((x - mean) / std_dev) ** 4 for x in data) / n - 3  # Excess kurtosis\n    return round(kurt, 6)",
      "test_cases": [
        {"input": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], false, true], "expected": {"valid": true, "statistics": {"count": 10, "original_count": 10, "invalid_count": 0, "mean": 5.5, "median": 5.5, "mode": {"values": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "frequency": 1, "is_multimodal": true}, "std_dev": 3.02765, "variance": 9.166667, "min": 1, "max": 10, "range": 9.0, "q1": 3.25, "q3": 7.75, "iqr": 4.5, "percentiles": {"p5": 1.45, "p10": 1.9, "p25": 3.25, "p50": 5.5, "p75": 7.75, "p90": 9.1, "p95": 9.55}}, "distribution": {"skewness": 0.0, "kurtosis": -1.2, "outlier_count": 0, "outlier_percentage": 0.0}, "outliers": [], "errors": []}},
        {"input": [[1, 2, 2, 3, 4, 100], true, true], "expected": {"valid": true, "statistics": {"count": 5, "original_count": 6, "invalid_count": 0, "mean": 2.4, "median": 2.0, "mode": {"values": [2], "frequency": 2, "is_multimodal": false}, "std_dev": 1.140175, "variance": 1.3, "min": 1, "max": 4, "range": 3.0, "q1": 1.5, "q3": 3.5, "iqr": 2.0, "percentiles": {"p5": 1.2, "p10": 1.4, "p25": 1.5, "p50": 2.0, "p75": 3.5, "p90": 3.8, "p95": 3.9}}, "distribution": {"skewness": 0.175436, "kurtosis": -1.911111, "outlier_count": 1, "outlier_percentage": 16.67}, "outliers": [{"value": 100, "type": "high", "distance_from_bound": 93.0}]}},
        {"input": [[], false, true], "expected": {"valid": false, "statistics": {}, "distribution": {}, "outliers": [], "errors": ["No data provided"]}},
        {"input": [["a", "b", "c"], false, true], "expected": {"valid": false, "statistics": {}, "distribution": {}, "outliers": [], "errors": ["No valid numeric data found"]}},
        {"input": [[5, 5, 5, 5], false, false], "expected": {"valid": true, "statistics": {"count": 4, "original_count": 4, "invalid_count": 0, "mean": 5.0, "median": 5.0, "mode": {"values": [5], "frequency": 4, "is_multimodal": false}, "std_dev": 0.0, "variance": 0.0, "min": 5, "max": 5, "range": 0.0, "q1": 5.0, "q3": 5.0, "iqr": 0.0, "percentiles": {"p5": 5.0, "p10": 5.0, "p25": 5.0, "p50": 5.0, "p75": 5.0, "p90": 5.0, "p95": 5.0}}, "distribution": {"skewness": 0, "kurtosis": 0, "outlier_count": 0, "outlier_percentage": 0.0}, "outliers": [], "errors": []}}
      ]
    },
    {
      "id": 137,
      "title": "Tabular Data Filter and Sorter",
      "statement": "Filter and sort tabular data based on multiple criteria. Support various filter operations (equals, contains, greater than, less than, between) and multi-column sorting with different orders. Handle different data types appropriately and provide filtering statistics.",
      "function_signature": "def filter_and_sort_data(data, filters=None, sort_columns=None, limit=None):",
      "categories": ["input_output_processing", "data_manipulation"],
      "python_constructs": ["data_filtering", "multi_key_sorting", "lambda_functions"],
      "hints": [
        "Support multiple filter operations: eq, ne, gt, lt, contains, between",
        "Implement multi-column sorting with ascending/descending options",
        "Handle different data types (string, numeric, date) appropriately"
      ],
      "solution": "def filter_and_sort_data(data, filters=None, sort_columns=None, limit=None):\n    result = {\n        'filtered_data': [],\n        'total_rows': 0,\n        'filtered_rows': 0,\n        'filter_statistics': {},\n        'errors': []\n    }\n    \n    if not data:\n        return result\n    \n    # Ensure data is list of dictionaries\n    if not isinstance(data, list) or not all(isinstance(row, dict) for row in data):\n        result['errors'].append('Data must be a list of dictionaries')\n        return result\n    \n    result['total_rows'] = len(data)\n    filtered_data = data[:]\n    \n    # Apply filters\n    if filters:\n        filter_stats = {}\n        \n        for filter_config in filters:\n            column = filter_config.get('column')\n            operation = filter_config.get('operation', 'eq')\n            value = filter_config.get('value')\n            \n            if not column:\n                result['errors'].append('Filter must specify column')\n                continue\n            \n            # Count rows before filtering\n            before_count = len(filtered_data)\n            \n            # Apply filter\n            filtered_data = apply_filter(filtered_data, column, operation, value)\n            \n            # Record statistics\n            after_count = len(filtered_data)\n            filter_stats[f'{column}_{operation}'] = {\n                'rows_before': before_count,\n                'rows_after': after_count,\n                'rows_filtered': before_count - after_count\n            }\n        \n        result['filter_statistics'] = filter_stats\n    \n    result['filtered_rows'] = len(filtered_data)\n    \n    # Apply sorting\n    if sort_columns and filtered_data:\n        try:\n            # Create sort key function for multiple columns\n            def sort_key(row):\n                keys = []\n                for sort_config in sort_columns:\n                    column = sort_config.get('column')\n                    reverse = sort_config.get('descending', False)\n                    \n                    if column in row:\n                        value = row[column]\n                        # Handle different data types for sorting\n                        if isinstance(value, str):\n                            # Case-insensitive string sorting\n                            sort_value = value.lower()\n                        elif value is None:\n                            # Put None values last\n                            sort_value = '' if not reverse else 'zzzzz'\n                        else:\n                            sort_value = value\n                        \n                        keys.append(sort_value)\n                    else:\n                        keys.append('' if not reverse else 'zzzzz')\n                \n                return keys\n            \n            # Sort with multiple keys\n            # Note: Python's sort is stable, so we sort by columns in reverse order\n            for sort_config in reversed(sort_columns):\n                column = sort_config.get('column')\n                descending = sort_config.get('descending', False)\n                \n                filtered_data.sort(\n                    key=lambda row: get_sort_value(row, column),\n                    reverse=descending\n                )\n        \n        except Exception as e:\n            result['errors'].append(f'Sorting error: {str(e)}')\n    \n    # Apply limit\n    if limit and isinstance(limit, int) and limit > 0:\n        filtered_data = filtered_data[:limit]\n    \n    result['filtered_data'] = filtered_data\n    return result\n\ndef apply_filter(data, column, operation, value):\n    \"\"\"Apply a single filter to the data\"\"\"\n    filtered = []\n    \n    for row in data:\n        if column not in row:\n            continue\n        \n        row_value = row[column]\n        \n        # Handle None values\n        if row_value is None:\n            if operation == 'eq' and value is None:\n                filtered.append(row)\n            elif operation == 'ne' and value is not None:\n                filtered.append(row)\n            continue\n        \n        # Apply filter operation\n        try:\n            if operation == 'eq':\n                if str(row_value).lower() == str(value).lower():\n                    filtered.append(row)\n            \n            elif operation == 'ne':\n                if str(row_value).lower() != str(value).lower():\n                    filtered.append(row)\n            \n            elif operation == 'contains':\n                if str(value).lower() in str(row_value).lower():\n                    filtered.append(row)\n            \n            elif operation == 'gt':\n                if isinstance(row_value, (int, float)) and isinstance(value, (int, float)):\n                    if row_value > value:\n                        filtered.append(row)\n                elif str(row_value) > str(value):\n                    filtered.append(row)\n            \n            elif operation == 'lt':\n                if isinstance(row_value, (int, float)) and isinstance(value, (int, float)):\n                    if row_value < value:\n                        filtered.append(row)\n                elif str(row_value) < str(value):\n                    filtered.append(row)\n            \n            elif operation == 'gte':\n                if isinstance(row_value, (int, float)) and isinstance(value, (int, float)):\n                    if row_value >= value:\n                        filtered.append(row)\n                elif str(row_value) >= str(value):\n                    filtered.append(row)\n            \n            elif operation == 'lte':\n                if isinstance(row_value, (int, float)) and isinstance(value, (int, float)):\n                    if row_value <= value:\n                        filtered.append(row)\n                elif str(row_value) <= str(value):\n                    filtered.append(row)\n            \n            elif operation == 'between':\n                if isinstance(value, (list, tuple)) and len(value) == 2:\n                    min_val, max_val = value\n                    if isinstance(row_value, (int, float)):\n                        if min_val <= row_value <= max_val:\n                            filtered.append(row)\n        \n        except (ValueError, TypeError):\n            # Skip rows that can't be compared\n            continue\n    \n    return filtered\n\ndef get_sort_value(row, column):\n    \"\"\"Get sortable value from row\"\"\"\n    if column not in row:\n        return ''\n    \n    value = row[column]\n    \n    if value is None:\n        return ''\n    elif isinstance(value, str):\n        return value.lower()\n    else:\n        return value",
      "test_cases": [
        {"input": [[{"name": "John", "age": 25, "city": "NYC"}, {"name": "Jane", "age": 30, "city": "LA"}, {"name": "Bob", "age": 20, "city": "NYC"}], [{"column": "city", "operation": "eq", "value": "NYC"}], [{"column": "age", "descending": false}], null], "expected": {"filtered_data": [{"name": "Bob", "age": 20, "city": "NYC"}, {"name": "John", "age": 25, "city": "NYC"}], "total_rows": 3, "filtered_rows": 2, "filter_statistics": {"city_eq": {"rows_before": 3, "rows_after": 2, "rows_filtered": 1}}, "errors": []}},
        {"input": [[{"name": "Alice", "score": 85}, {"name": "Bob", "score": 92}, {"name": "Charlie", "score": 78}], [{"column": "score", "operation": "gt", "value": 80}], [{"column": "score", "descending": true}], 1], "expected": {"filtered_data": [{"name": "Bob", "score": 92}], "total_rows": 3, "filtered_rows": 2, "filter_statistics": {"score_gt": {"rows_before": 3, "rows_after": 2, "rows_filtered": 1}}, "errors": []}},
        {"input": [[{"product": "Apple", "price": 1.5}, {"product": "Banana", "price": 0.8}, {"product": "Orange", "price": 2.0}], [{"column": "product", "operation": "contains", "value": "a"}], [{"column": "price", "descending": false}], null], "expected": {"filtered_data": [{"product": "Banana", "price": 0.8}, {"product": "Orange", "price": 2.0}], "total_rows": 3, "filtered_rows": 2, "filter_statistics": {"product_contains": {"rows_before": 3, "rows_after": 2, "rows_filtered": 1}}, "errors": []}},
        {"input": [[], null, null, null], "expected": {"filtered_data": [], "total_rows": 0, "filtered_rows": 0, "filter_statistics": {}, "errors": []}},
        {"input": ["invalid_data"], null, null, null], "expected": {"filtered_data": [], "total_rows": 0, "filtered_rows": 0, "filter_statistics": {}, "errors": ["Data must be a list of dictionaries"]}}
      ]
    },
    {
      "id": 138,
      "title": "Report Generator from Raw Data",
      "statement": "Generate formatted reports from raw data with customizable templates, aggregations, and output formats (text, HTML, markdown). Support grouping, summarization, and chart generation recommendations. Include executive summary and detailed analysis sections.",
      "function_signature": "def generate_data_report(data, report_config):",
      "categories": ["input_output_processing", "report_generation"],
      "python_constructs": ["template_processing", "data_aggregation", "string_formatting"],
      "hints": [
        "Support multiple output formats with appropriate formatting",
        "Implement data grouping and aggregation functions",
        "Generate executive summary with key metrics and insights"
      ],
      "solution": "def generate_data_report(data, report_config):\n    from collections import defaultdict, Counter\n    from datetime import datetime\n    \n    result = {\n        'success': False,\n        'report': '',\n        'statistics': {},\n        'errors': []\n    }\n    \n    if not data or not report_config:\n        result['errors'].append('Data and report configuration required')\n        return result\n    \n    try:\n        # Extract configuration\n        title = report_config.get('title', 'Data Report')\n        output_format = report_config.get('format', 'text')\n        include_summary = report_config.get('include_summary', True)\n        group_by = report_config.get('group_by')\n        aggregations = report_config.get('aggregations', [])\n        sort_by = report_config.get('sort_by')\n        \n        # Prepare data\n        if isinstance(data, dict):\n            data = [data]\n        \n        # Generate report sections\n        sections = []\n        \n        # Header\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        header = generate_header(title, timestamp, output_format)\n        sections.append(header)\n        \n        # Executive Summary\n        if include_summary:\n            summary = generate_executive_summary(data, output_format)\n            sections.append(summary)\n        \n        # Data Analysis\n        if group_by:\n            grouped_analysis = generate_grouped_analysis(data, group_by, aggregations, output_format)\n            sections.append(grouped_analysis)\n        \n        # Detailed Data\n        detailed_section = generate_detailed_section(data, sort_by, output_format)\n        sections.append(detailed_section)\n        \n        # Aggregation Results\n        if aggregations:\n            agg_section = generate_aggregation_section(data, aggregations, output_format)\n            sections.append(agg_section)\n        \n        # Combine sections\n        if output_format == 'html':\n            report = f'<html><body>{\"\".join(sections)}</body></html>'\n        else:\n            report = '\\n\\n'.join(sections)\n        \n        # Calculate statistics\n        stats = calculate_report_statistics(data)\n        \n        result['success'] = True\n        result['report'] = report\n        result['statistics'] = stats\n        \n    except Exception as e:\n        result['errors'].append(f'Report generation error: {str(e)}')\n    \n    return result\n\ndef generate_header(title, timestamp, format_type):\n    \"\"\"Generate report header\"\"\"\n    if format_type == 'html':\n        return f'<h1>{title}</h1><p><em>Generated: {timestamp}</em></p>'\n    elif format_type == 'markdown':\n        return f'# {title}\\n\\n*Generated: {timestamp}*'\n    else:\n        return f'{title}\\n{\"=\" * len(title)}\\nGenerated: {timestamp}'\n\ndef generate_executive_summary(data, format_type):\n    \"\"\"Generate executive summary section\"\"\"\n    total_records = len(data)\n    \n    # Extract numeric columns for summary\n    numeric_data = defaultdict(list)\n    for record in data:\n        for key, value in record.items():\n            if isinstance(value, (int, float)):\n                numeric_data[key].append(value)\n    \n    summary_stats = {}\n    for column, values in numeric_data.items():\n        if values:\n            summary_stats[column] = {\n                'avg': sum(values) / len(values),\n                'min': min(values),\n                'max': max(values),\n                'total': sum(values)\n            }\n    \n    if format_type == 'html':\n        html = '<h2>Executive Summary</h2>'\n        html += f'<p>Total Records: {total_records}</p>'\n        \n        if summary_stats:\n            html += '<h3>Key Metrics</h3><ul>'\n            for column, stats in summary_stats.items():\n                html += f'<li><strong>{column}</strong>: Avg={stats[\"avg\"]:.2f}, Min={stats[\"min\"]}, Max={stats[\"max\"]}, Total={stats[\"total\"]:.2f}</li>'\n            html += '</ul>'\n        \n        return html\n    \n    elif format_type == 'markdown':\n        md = '## Executive Summary\\n\\n'\n        md += f'**Total Records:** {total_records}\\n\\n'\n        \n        if summary_stats:\n            md += '### Key Metrics\\n\\n'\n            for column, stats in summary_stats.items():\n                md += f'- **{column}**: Avg={stats[\"avg\"]:.2f}, Min={stats[\"min\"]}, Max={stats[\"max\"]}, Total={stats[\"total\"]:.2f}\\n'\n        \n        return md\n    \n    else:\n        text = 'EXECUTIVE SUMMARY\\n' + '-' * 17 + '\\n'\n        text += f'Total Records: {total_records}\\n'\n        \n        if summary_stats:\n            text += '\\nKey Metrics:\\n'\n            for column, stats in summary_stats.items():\n                text += f'  {column}: Avg={stats[\"avg\"]:.2f}, Min={stats[\"min\"]}, Max={stats[\"max\"]}, Total={stats[\"total\"]:.2f}\\n'\n        \n        return text\n\ndef generate_grouped_analysis(data, group_by, aggregations, format_type):\n    \"\"\"Generate grouped analysis section\"\"\"\n    groups = defaultdict(list)\n    \n    for record in data:\n        group_key = record.get(group_by, 'Unknown')\n        groups[group_key].append(record)\n    \n    if format_type == 'html':\n        html = f'<h2>Analysis by {group_by}</h2>'\n        for group_name, group_data in groups.items():\n            html += f'<h3>{group_name} ({len(group_data)} records)</h3>'\n            # Add group-specific statistics here\n        return html\n    \n    elif format_type == 'markdown':\n        md = f'## Analysis by {group_by}\\n\\n'\n        for group_name, group_data in groups.items():\n            md += f'### {group_name} ({len(group_data)} records)\\n\\n'\n        return md\n    \n    else:\n        text = f'ANALYSIS BY {group_by.upper()}\\n' + '-' * (12 + len(group_by)) + '\\n'\n        for group_name, group_data in groups.items():\n            text += f'\\n{group_name}: {len(group_data)} records\\n'\n        return text\n\ndef generate_detailed_section(data, sort_by, format_type):\n    \"\"\"Generate detailed data section\"\"\"\n    # Sort data if specified\n    if sort_by and data:\n        try:\n            data = sorted(data, key=lambda x: x.get(sort_by, 0))\n        except:\n            pass  # If sorting fails, use original order\n    \n    if format_type == 'html':\n        if not data:\n            return '<h2>Detailed Data</h2><p>No data available</p>'\n        \n        html = '<h2>Detailed Data</h2><table border=\"1\">'\n        \n        # Header\n        headers = list(data[0].keys())\n        html += '<tr>' + ''.join(f'<th>{h}</th>' for h in headers) + '</tr>'\n        \n        # Rows (limit to first 10 for readability)\n        for record in data[:10]:\n            html += '<tr>' + ''.join(f'<td>{record.get(h, \"\")}</td>' for h in headers) + '</tr>'\n        \n        if len(data) > 10:\n            html += f'<tr><td colspan=\"{len(headers)}\">... and {len(data) - 10} more records</td></tr>'\n        \n        html += '</table>'\n        return html\n    \n    else:\n        text = 'DETAILED DATA\\n' + '-' * 13 + '\\n'\n        \n        if not data:\n            text += 'No data available\\n'\n            return text\n        \n        # Show first few records\n        for i, record in enumerate(data[:5]):\n            text += f'\\nRecord {i+1}:\\n'\n            for key, value in record.items():\n                text += f'  {key}: {value}\\n'\n        \n        if len(data) > 5:\n            text += f'\\n... and {len(data) - 5} more records\\n'\n        \n        return text\n\ndef generate_aggregation_section(data, aggregations, format_type):\n    \"\"\"Generate aggregation results section\"\"\"\n    results = {}\n    \n    for agg_config in aggregations:\n        column = agg_config.get('column')\n        operation = agg_config.get('operation', 'sum')\n        \n        if not column:\n            continue\n        \n        values = [record.get(column) for record in data if isinstance(record.get(column), (int, float))]\n        \n        if values:\n            if operation == 'sum':\n                results[f'{column}_sum'] = sum(values)\n            elif operation == 'avg':\n                results[f'{column}_avg'] = sum(values) / len(values)\n            elif operation == 'count':\n                results[f'{column}_count'] = len(values)\n            elif operation == 'min':\n                results[f'{column}_min'] = min(values)\n            elif operation == 'max':\n                results[f'{column}_max'] = max(values)\n    \n    if format_type == 'html':\n        html = '<h2>Aggregation Results</h2><ul>'\n        for key, value in results.items():\n            html += f'<li><strong>{key}:</strong> {value}</li>'\n        html += '</ul>'\n        return html\n    \n    else:\n        text = 'AGGREGATION RESULTS\\n' + '-' * 19 + '\\n'\n        for key, value in results.items():\n            text += f'{key}: {value}\\n'\n        return text\n\ndef calculate_report_statistics(data):\n    \"\"\"Calculate overall report statistics\"\"\"\n    return {\n        'total_records': len(data),\n        'total_fields': len(data[0].keys()) if data else 0,\n        'numeric_fields': sum(1 for record in data[:1] if data for value in record.values() if isinstance(value, (int, float))),\n        'text_fields': sum(1 for record in data[:1] if data for value in record.values() if isinstance(value, str))\n    }",
      "test_cases": [
        {"input": [[{"name": "John", "age": 25, "salary": 50000}, {"name": "Jane", "age": 30, "salary": 60000}], {"title": "Employee Report", "format": "text", "include_summary": true}], "expected": {"success": true, "report": "Employee Report\\n===============\\nGenerated: 2023-10-10 14:30:15\\n\\nEXECUTIVE SUMMARY\\n-----------------\\nTotal Records: 2\\n\\nKey Metrics:\\n  age: Avg=27.50, Min=25, Max=30, Total=55.00\\n  salary: Avg=55000.00, Min=50000, Max=60000, Total=110000.00\\n\\nDETAILED DATA\\n-------------\\n\\nRecord 1:\\n  name: John\\n  age: 25\\n  salary: 50000\\n\\nRecord 2:\\n  name: Jane\\n  age: 30\\n  salary: 60000", "statistics": {"total_records": 2, "total_fields": 3, "numeric_fields": 2, "text_fields": 1}, "errors": []}},
        {"input": [[{"category": "A", "value": 100}, {"category": "B", "value": 200}, {"category": "A", "value": 150}], {"title": "Category Analysis", "format": "html", "group_by": "category", "aggregations": [{"column": "value", "operation": "sum"}]}], "expected": {"success": true, "report": "<html><body><h1>Category Analysis</h1><p><em>Generated: 2023-10-10 14:30:15</em></p><h2>Executive Summary</h2><p>Total Records: 3</p><h3>Key Metrics</h3><ul><li><strong>value</strong>: Avg=150.00, Min=100, Max=200, Total=450.00</li></ul><h2>Analysis by category</h2><h3>A (2 records)</h3><h3>B (1 records)</h3><h2>Detailed Data</h2><table border=\"1\"><tr><th>category</th><th>value</th></tr><tr><td>A</td><td>100</td></tr><tr><td>B</td><td>200</td></tr><tr><td>A</td><td>150</td></tr></table><h2>Aggregation Results</h2><ul><li><strong>value_sum:</strong> 450</li></ul></body></html>", "statistics": {"total_records": 3, "total_fields": 2, "numeric_fields": 1, "text_fields": 1}, "errors": []}},
        {"input": [[], {"title": "Empty Report"}], "expected": {"success": false, "report": "", "statistics": {}, "errors": ["Data and report configuration required"]}},
        {"input": [[{"item": "Widget", "price": 9.99, "quantity": 5}], {"title": "Inventory Report", "format": "markdown", "aggregations": [{"column": "price", "operation": "avg"}, {"column": "quantity", "operation": "sum"}]}], "expected": {"success": true, "report": "# Inventory Report\\n\\n*Generated: 2023-10-10 14:30:15*\\n\\n## Executive Summary\\n\\n**Total Records:** 1\\n\\n### Key Metrics\\n\\n- **price**: Avg=9.99, Min=9.99, Max=9.99, Total=9.99\\n- **quantity**: Avg=5.00, Min=5, Max=5, Total=5.00\\n\\nDETAILED DATA\\n-------------\\n\\nRecord 1:\\n  item: Widget\\n  price: 9.99\\n  quantity: 5\\n\\nAGGREGATION RESULTS\\n-------------------\\nprice_avg: 9.99\\nquantity_sum: 5", "statistics": {"total_records": 1, "total_fields": 3, "numeric_fields": 2, "text_fields": 1}, "errors": []}},
        {"input": [null, {"title": "Test"}], "expected": {"success": false, "report": "", "statistics": {}, "errors": ["Data and report configuration required"]}}
      ]
    }
  ]
}            except ValueError:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Invalid {data_type} format')\n                result['valid'] = False\n        \n        result['field_results'][field_name] = field_result\n    \n    return result\n\ndef validate_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}{
  "problems": [
    {
      "id": 129,
      "title": "CSV File Parser and Data Extractor",
      "statement": "Parse a CSV file content and extract specific columns with data validation. Handle various delimiters (comma, semicolon, tab), quoted fields, and missing values. Return structured data with column headers and rows. For example, parse 'Name,Age,City\\nJohn,25,NYC\\nJane,30,LA' and extract specific columns with type conversion.",
      "function_signature": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):",
      "categories": ["input_output_processing", "data_parsing"],
      "python_constructs": ["csv_parsing", "string_manipulation", "data_validation"],
      "hints": [
        "Use csv module or manual parsing to handle different delimiters",
        "Handle quoted fields that may contain delimiter characters",
        "Implement type conversion for numeric columns and date parsing"
      ],
      "solution": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):\n    import csv\n    from io import StringIO\n    \n    if not csv_content:\n        return {'headers': [], 'rows': [], 'error': None}\n    \n    try:\n        # Parse CSV content\n        csv_reader = csv.reader(StringIO(csv_content), delimiter=delimiter)\n        rows = list(csv_reader)\n        \n        if not rows:\n            return {'headers': [], 'rows': [], 'error': None}\n        \n        headers = rows[0]\n        data_rows = rows[1:]\n        \n        # Extract specific columns if requested\n        if extract_columns:\n            try:\n                # Find column indices\n                column_indices = []\n                for col in extract_columns:\n                    if isinstance(col, str):\n                        if col in headers:\n                            column_indices.append(headers.index(col))\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column \"{col}\" not found'}\n                    elif isinstance(col, int):\n                        if 0 <= col < len(headers):\n                            column_indices.append(col)\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column index {col} out of range'}\n                \n                # Extract columns\n                filtered_headers = [headers[i] for i in column_indices]\n                filtered_rows = []\n                \n                for row in data_rows:\n                    filtered_row = []\n                    for i in column_indices:\n                        if i < len(row):\n                            filtered_row.append(row[i])\n                        else:\n                            filtered_row.append('')  # Missing value\n                    filtered_rows.append(filtered_row)\n                \n                headers = filtered_headers\n                data_rows = filtered_rows\n            \n            except Exception as e:\n                return {'headers': [], 'rows': [], 'error': f'Column extraction error: {str(e)}'}\n        \n        # Apply type conversions\n        if convert_types:\n            converted_rows = []\n            for row in data_rows:\n                converted_row = []\n                for i, value in enumerate(row):\n                    if i < len(headers) and headers[i] in convert_types:\n                        try:\n                            converter = convert_types[headers[i]]\n                            if converter == 'int':\n                                converted_row.append(int(value) if value.strip() else None)\n                            elif converter == 'float':\n                                converted_row.append(float(value) if value.strip() else None)\n                            elif converter == 'bool':\n                                converted_row.append(value.lower() in ['true', '1', 'yes', 'on'])\n                            else:\n                                converted_row.append(value)\n                        except (ValueError, AttributeError):\n                            converted_row.append(None)  # Conversion failed\n                    else:\n                        converted_row.append(value)\n                converted_rows.append(converted_row)\n            data_rows = converted_rows\n        \n        return {\n            'headers': headers,\n            'rows': data_rows,\n            'row_count': len(data_rows),\n            'column_count': len(headers),\n            'error': None\n        }\n    \n    except Exception as e:\n        return {'headers': [], 'rows': [], 'error': f'Parsing error: {str(e)}'}"
      ],
      "test_cases": [
        {"input": ["Name,Age,City\\nJohn,25,NYC\\nJane,30,LA", ",", null, null], "expected": {"headers": ["Name", "Age", "City"], "rows": [["John", "25", "NYC"], ["Jane", "30", "LA"]], "row_count": 2, "column_count": 3, "error": null}},
        {"input": ["Name;Age;Salary\\nJohn;25;50000\\nJane;30;60000", ";", ["Name", "Salary"], {"Salary": "int"}], "expected": {"headers": ["Name", "Salary"], "rows": [["John", 50000], ["Jane", 60000]], "row_count": 2, "column_count": 2, "error": null}},
        {"input": ["A,B,C\\n1,2,3", ",", [0, 2], {"A": "int", "C": "int"}], "expected": {"headers": ["A", "C"], "rows": [[1, 3]], "row_count": 1, "column_count": 2, "error": null}},
        {"input": [""], "expected": {"headers": [], "rows": [], "error": null}},
        {"input": ["Name,Age\\nJohn,25", ",", ["NonExistent"], null], "expected": {"headers": [], "rows": [], "error": "Column \"NonExistent\" not found"}}
      ]
    },
    {
      "id": 130,
      "title": "File Content Word and Line Counter",
      "statement": "Count words, lines, characters, and paragraphs in text content. Provide detailed statistics including average word length, sentence count, and most frequent words. Handle different text encodings and empty lines. Return comprehensive text analysis results.",
      "function_signature": "def analyze_text_content(content, include_word_freq=True, top_words=10):",
      "categories": ["input_output_processing", "text_analysis"],
      "python_constructs": ["text_processing", "regular_expressions", "frequency_analysis"],
      "hints": [
        "Use regular expressions to identify words, sentences, and paragraphs",
        "Handle different definitions of word boundaries and punctuation",
        "Calculate frequency distribution and sort by occurrence count"
      ],
      "solution": "def analyze_text_content(content, include_word_freq=True, top_words=10):\n    import re\n    from collections import Counter\n    \n    if not isinstance(content, str):\n        return None\n    \n    if not content:\n        return {\n            'characters': 0,\n            'characters_no_spaces': 0,\n            'words': 0,\n            'lines': 0,\n            'paragraphs': 0,\n            'sentences': 0,\n            'avg_word_length': 0.0,\n            'word_frequency': {} if include_word_freq else None\n        }\n    \n    # Basic counts\n    char_count = len(content)\n    char_count_no_spaces = len(content.replace(' ', '').replace('\\t', '').replace('\\n', '').replace('\\r', ''))\n    \n    # Line count\n    lines = content.split('\\n')\n    line_count = len(lines)\n    \n    # Paragraph count (separated by empty lines)\n    paragraphs = re.split(r'\\n\\s*\\n', content.strip())\n    paragraph_count = len([p for p in paragraphs if p.strip()])\n    \n    # Word extraction and counting\n    words = re.findall(r\"\\b\\w+\\b\", content.lower())\n    word_count = len(words)\n    \n    # Average word length\n    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0.0\n    \n    # Sentence count (approximate)\n    sentences = re.split(r'[.!?]+', content)\n    sentence_count = len([s for s in sentences if s.strip()])\n    \n    result = {\n        'characters': char_count,\n        'characters_no_spaces': char_count_no_spaces,\n        'words': word_count,\n        'lines': line_count,\n        'paragraphs': paragraph_count,\n        'sentences': sentence_count,\n        'avg_word_length': round(avg_word_length, 2)\n    }\n    \n    # Word frequency analysis\n    if include_word_freq and words:\n        word_freq = Counter(words)\n        top_word_list = word_freq.most_common(top_words)\n        result['word_frequency'] = {\n            'total_unique_words': len(word_freq),\n            'top_words': top_word_list,\n            'most_common_word': top_word_list[0] if top_word_list else None\n        }\n    else:\n        result['word_frequency'] = None\n    \n    return result",
      "test_cases": [
        {"input": ["Hello world! This is a test.\\nAnother line here.", true, 5], "expected": {"characters": 46, "characters_no_spaces": 37, "words": 9, "lines": 2, "paragraphs": 1, "sentences": 2, "avg_word_length": 4.0, "word_frequency": {"total_unique_words": 9, "top_words": [["hello", 1], ["world", 1], ["this", 1], ["is", 1], ["a", 1]], "most_common_word": ["hello", 1]}}},
        {"input": [""], "expected": {"characters": 0, "characters_no_spaces": 0, "words": 0, "lines": 0, "paragraphs": 0, "sentences": 0, "avg_word_length": 0.0, "word_frequency": {}}},
        {"input": ["Word word WORD!", false, 3], "expected": {"characters": 14, "characters_no_spaces": 11, "words": 3, "lines": 1, "paragraphs": 1, "sentences": 1, "avg_word_length": 4.0, "word_frequency": null}},
        {"input": ["Line 1\\n\\nParagraph 2\\nStill paragraph 2", true, 3], "expected": {"characters": 35, "characters_no_spaces": 25, "words": 6, "lines": 4, "paragraphs": 2, "sentences": 1, "avg_word_length": 4.17, "word_frequency": {"total_unique_words": 6, "top_words": [["line", 1], ["1", 1], ["paragraph", 2]], "most_common_word": ["paragraph", 2]}}},
        {"input": ["Hello! How are you? I am fine.", true, 10], "expected": {"characters": 30, "characters_no_spaces": 24, "words": 7, "lines": 1, "paragraphs": 1, "sentences": 3, "avg_word_length": 3.0, "word_frequency": {"total_unique_words": 7, "top_words": [["hello", 1], ["how", 1], ["are", 1], ["you", 1], ["i", 1], ["am", 1], ["fine", 1]], "most_common_word": ["hello", 1]}}}
      ]
    },
    {
      "id": 131,
      "title": "JSON Data Parser and Validator",
      "statement": "Parse and validate JSON data with schema checking, nested object extraction, and data transformation. Handle malformed JSON, validate data types, and extract specific fields using JSONPath-like syntax. Return parsed data with validation results and extracted fields.",
      "function_signature": "def parse_json_data(json_string, schema=None, extract_paths=None):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["json_parsing", "data_validation", "nested_data_access"],
      "hints": [
        "Use json module for parsing and handle JSONDecodeError exceptions",
        "Implement schema validation for required fields and data types",
        "Support dot notation for nested field extraction"
      ],
      "solution": "def parse_json_data(json_string, schema=None, extract_paths=None):\n    import json\n    \n    result = {\n        'valid': False,\n        'data': None,\n        'errors': [],\n        'extracted_fields': {},\n        'schema_valid': True\n    }\n    \n    # Parse JSON\n    try:\n        data = json.loads(json_string)\n        result['valid'] = True\n        result['data'] = data\n    except json.JSONDecodeError as e:\n        result['errors'].append(f'JSON parsing error: {str(e)}')\n        return result\n    except Exception as e:\n        result['errors'].append(f'Unexpected error: {str(e)}')\n        return result\n    \n    # Schema validation\n    if schema:\n        schema_errors = validate_schema(data, schema)\n        if schema_errors:\n            result['schema_valid'] = False\n            result['errors'].extend(schema_errors)\n    \n    # Extract specific fields\n    if extract_paths:\n        for path_name, path in extract_paths.items():\n            try:\n                extracted_value = extract_nested_value(data, path)\n                result['extracted_fields'][path_name] = extracted_value\n            except Exception as e:\n                result['errors'].append(f'Extraction error for \"{path_name}\": {str(e)}')\n                result['extracted_fields'][path_name] = None\n    \n    return result\n\ndef validate_schema(data, schema):\n    errors = []\n    \n    # Check required fields\n    if 'required' in schema:\n        for field in schema['required']:\n            if field not in data:\n                errors.append(f'Missing required field: {field}')\n    \n    # Check field types\n    if 'properties' in schema:\n        for field, field_schema in schema['properties'].items():\n            if field in data:\n                expected_type = field_schema.get('type')\n                actual_value = data[field]\n                \n                if expected_type == 'string' and not isinstance(actual_value, str):\n                    errors.append(f'Field \"{field}\" should be string, got {type(actual_value).__name__}')\n                elif expected_type == 'number' and not isinstance(actual_value, (int, float)):\n                    errors.append(f'Field \"{field}\" should be number, got {type(actual_value).__name__}')\n                elif expected_type == 'integer' and not isinstance(actual_value, int):\n                    errors.append(f'Field \"{field}\" should be integer, got {type(actual_value).__name__}')\n                elif expected_type == 'boolean' and not isinstance(actual_value, bool):\n                    errors.append(f'Field \"{field}\" should be boolean, got {type(actual_value).__name__}')\n                elif expected_type == 'array' and not isinstance(actual_value, list):\n                    errors.append(f'Field \"{field}\" should be array, got {type(actual_value).__name__}')\n                elif expected_type == 'object' and not isinstance(actual_value, dict):\n                    errors.append(f'Field \"{field}\" should be object, got {type(actual_value).__name__}')\n    \n    return errors\n\ndef extract_nested_value(data, path):\n    \"\"\"Extract value using dot notation path like 'user.profile.name'\"\"\"\n    keys = path.split('.')\n    current = data\n    \n    for key in keys:\n        if isinstance(current, dict) and key in current:\n            current = current[key]\n        elif isinstance(current, list) and key.isdigit():\n            index = int(key)\n            if 0 <= index < len(current):\n                current = current[index]\n            else:\n                raise KeyError(f'List index {index} out of range')\n        else:\n            raise KeyError(f'Key \"{key}\" not found in path \"{path}\"')\n    \n    return current",
      "test_cases": [
        {"input": ["{\"name\": \"John\", \"age\": 30, \"city\": \"NYC\"}", null, null], "expected": {"valid": true, "data": {"name": "John", "age": 30, "city": "NYC"}, "errors": [], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": \"John\", \"age\": 30}", {"required": ["name", "age"], "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}, {"user_name": "name", "user_age": "age"}], "expected": {"valid": true, "data": {"name": "John", "age": 30}, "errors": [], "extracted_fields": {"user_name": "John", "user_age": 30}, "schema_valid": true}},
        {"input": ["{\"user\": {\"profile\": {\"name\": \"Alice\"}}}", null, {"nested_name": "user.profile.name"}], "expected": {"valid": true, "data": {"user": {"profile": {"name": "Alice"}}}, "errors": [], "extracted_fields": {"nested_name": "Alice"}, "schema_valid": true}},
        {"input": ["invalid json", null, null], "expected": {"valid": false, "data": null, "errors": ["JSON parsing error: Expecting value: line 1 column 1 (char 0)"], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": 123}", {"required": ["name"], "properties": {"name": {"type": "string"}}}, null], "expected": {"valid": true, "data": {"name": 123}, "errors": ["Field \"name\" should be string, got int"], "extracted_fields": {}, "schema_valid": false}}
      ]
    },
    {
      "id": 132,
      "title": "Command Line Arguments Processor",
      "statement": "Process command line arguments with support for flags, options with values, positional arguments, and help generation. Handle short (-h) and long (--help) options, validate required arguments, and provide default values. Return parsed arguments dictionary with validation results.",
      "function_signature": "def process_command_args(args_list, arg_definitions):",
      "categories": ["input_output_processing", "argument_parsing"],
      "python_constructs": ["argument_parsing", "command_line_interface", "validation_logic"],
      "hints": [
        "Parse arguments into flags (boolean), options (key-value), and positional arguments",
        "Support both short (-h) and long (--help) argument formats",
        "Validate required arguments and apply default values"
      ],
      "solution": "def process_command_args(args_list, arg_definitions):\n    result = {\n        'parsed_args': {},\n        'positional_args': [],\n        'errors': [],\n        'help_requested': False\n    }\n    \n    if not args_list:\n        args_list = []\n    \n    # Check for help request\n    if '-h' in args_list or '--help' in args_list:\n        result['help_requested'] = True\n        result['help_text'] = generate_help_text(arg_definitions)\n        return result\n    \n    # Initialize with default values\n    for arg_name, arg_config in arg_definitions.items():\n        if 'default' in arg_config:\n            result['parsed_args'][arg_name] = arg_config['default']\n    \n    i = 0\n    while i < len(args_list):\n        arg = args_list[i]\n        \n        if arg.startswith('--'):\n            # Long option\n            option_name = arg[2:]\n            arg_config = find_arg_config(option_name, arg_definitions, 'long')\n            \n            if not arg_config:\n                result['errors'].append(f'Unknown option: {arg}')\n                i += 1\n                continue\n            \n            arg_name, config = arg_config\n            \n            if config.get('type') == 'flag':\n                result['parsed_args'][arg_name] = True\n                i += 1\n            else:\n                # Option with value\n                if i + 1 >= len(args_list):\n                    result['errors'].append(f'Option {arg} requires a value')\n                    i += 1\n                    continue\n                \n                value = args_list[i + 1]\n                converted_value = convert_arg_value(value, config.get('value_type', 'string'))\n                \n                if converted_value is None and config.get('value_type') != 'string':\n                    result['errors'].append(f'Invalid {config.get(\"value_type\")} value for {arg}: {value}')\n                else:\n                    result['parsed_args'][arg_name] = converted_value if converted_value is not None else value\n                \n                i += 2\n        \n        elif arg.startswith('-') and len(arg) > 1:\n            # Short option(s)\n            for char in arg[1:]:\n                arg_config = find_arg_config(char, arg_definitions, 'short')\n                \n                if not arg_config:\n                    result['errors'].append(f'Unknown option: -{char}')\n                    continue\n                \n                arg_name, config = arg_config\n                \n                if config.get('type') == 'flag':\n                    result['parsed_args'][arg_name] = True\n            i += 1\n        \n        else:\n            # Positional argument\n            result['positional_args'].append(arg)\n            i += 1\n    \n    # Validate required arguments\n    for arg_name, arg_config in arg_definitions.items():\n        if arg_config.get('required', False) and arg_name not in result['parsed_args']:\n            result['errors'].append(f'Required argument missing: {arg_name}')\n    \n    return result\n\ndef find_arg_config(option, arg_definitions, option_type):\n    for arg_name, config in arg_definitions.items():\n        if option_type == 'long' and config.get('long') == option:\n            return (arg_name, config)\n        elif option_type == 'short' and config.get('short') == option:\n            return (arg_name, config)\n    return None\n\ndef convert_arg_value(value, value_type):\n    try:\n        if value_type == 'int':\n            return int(value)\n        elif value_type == 'float':\n            return float(value)\n        elif value_type == 'bool':\n            return value.lower() in ['true', '1', 'yes', 'on']\n        else:\n            return value\n    except (ValueError, AttributeError):\n        return None\n\ndef generate_help_text(arg_definitions):\n    help_lines = ['Available options:']\n    \n    for arg_name, config in arg_definitions.items():\n        line_parts = []\n        \n        if config.get('short'):\n            line_parts.append(f'-{config[\"short\"]}')\n        \n        if config.get('long'):\n            line_parts.append(f'--{config[\"long\"]}')\n        \n        option_text = ', '.join(line_parts)\n        \n        if config.get('description'):\n            option_text += f': {config[\"description\"]}'\n        \n        if config.get('required'):\n            option_text += ' (required)'\n        \n        if 'default' in config:\n            option_text += f' (default: {config[\"default\"]})')\n        \n        help_lines.append(f'  {option_text}')\n    \n    return '\\n'.join(help_lines)",
      "test_cases": [
        {"input": [["--input", "file.txt", "-v", "--output", "result.txt"], {"input_file": {"long": "input", "required": true}, "verbose": {"short": "v", "type": "flag"}, "output_file": {"long": "output", "default": "output.txt"}}], "expected": {"parsed_args": {"input_file": "file.txt", "verbose": true, "output_file": "result.txt"}, "positional_args": [], "errors": [], "help_requested": false}},
        {"input": [["-h"], {"verbose": {"short": "v", "type": "flag"}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": [], "help_requested": true, "help_text": "Available options:\\n  -v: (required)"}},
        {"input": [["--count", "5", "file1", "file2"], {"count": {"long": "count", "value_type": "int", "default": 1}}], "expected": {"parsed_args": {"count": 5}, "positional_args": ["file1", "file2"], "errors": [], "help_requested": false}},
        {"input": [["--missing"], {"required_arg": {"long": "required", "required": true}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": ["Unknown option: --missing", "Required argument missing: required_arg"], "help_requested": false}},
        {"input": [[], {"default_val": {"long": "default", "default": "test"}}], "expected": {"parsed_args": {"default_val": "test"}, "positional_args": [], "errors": [], "help_requested": false}}
      ]
    },
    {
      "id": 133,
      "title": "Data Validation Engine",
      "statement": "Validate various data types including email addresses, phone numbers, credit card numbers, URLs, and custom patterns. Return detailed validation results with specific error messages and suggestions for correction. Support multiple validation rules per field.",
      "function_signature": "def validate_data_fields(data, validation_rules):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["regular_expressions", "validation_patterns", "error_handling"],
      "hints": [
        "Use regular expressions for pattern matching (email, phone, URL)",
        "Implement Luhn algorithm for credit card validation",
        "Provide specific error messages and correction suggestions"
      ],
      "solution": "def validate_data_fields(data, validation_rules):\n    import re\n    \n    result = {\n        'valid': True,\n        'field_results': {},\n        'errors': [],\n        'warnings': []\n    }\n    \n    for field_name, rules in validation_rules.items():\n        field_result = {\n            'valid': True,\n            'value': data.get(field_name),\n            'errors': [],\n            'warnings': []\n        }\n        \n        field_value = data.get(field_name)\n        \n        # Check if field is required\n        if rules.get('required', False) and (field_value is None or field_value == ''):\n            field_result['valid'] = False\n            field_result['errors'].append('Field is required')\n            result['valid'] = False\n        \n        # Skip further validation if field is empty and not required\n        if field_value is None or field_value == '':\n            result['field_results'][field_name] = field_result\n            continue\n        \n        # Validate based on data type\n        data_type = rules.get('type', 'string')\n        \n        if data_type == 'email':\n            if not validate_email(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid email format')\n                result['valid'] = False\n        \n        elif data_type == 'phone':\n            phone_result = validate_phone(field_value)\n            if not phone_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(phone_result['error'])\n                result['valid'] = False\n        \n        elif data_type == 'credit_card':\n            cc_result = validate_credit_card(field_value)\n            if not cc_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(cc_result['error'])\n                result['valid'] = False\n            else:\n                field_result['card_type'] = cc_result.get('card_type')\n        \n        elif data_type == 'url':\n            if not validate_url(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid URL format')\n                result['valid'] = False\n        \n        elif data_type == 'pattern':\n            pattern = rules.get('pattern')\n            if pattern and not re.match(pattern, str(field_value)):\n                field_result['valid'] = False\n                field_result['errors'].append(f'Does not match required pattern: {pattern}')\n                result['valid'] = False\n        \n        # Length validation\n        if 'min_length' in rules:\n            if len(str(field_value)) < rules['min_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Minimum length is {rules[\"min_length\"]}')\n                result['valid'] = False\n        \n        if 'max_length' in rules:\n            if len(str(field_value)) > rules['max_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Maximum length is {rules[\"max_length\"]}')\n                result['valid'] = False\n        \n        # Range validation for numbers\n        if data_type in ['int', 'float']:\n            try:\n                num_value = float(field_value)\n                if 'min_value' in rules and num_value < rules['min_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Minimum value is {rules[\"min_value\"]}')\n                    result['valid'] = False\n                \n                if 'max_value' in rules and num_value > rules['max_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Maximum value is {rules[\"max_value\"]}')\n                    result['valid'] = False\n            except ValueError:\n                field_result['valid'] =\n    return re.match(pattern, email) is not None\n\ndef validate_phone(phone):\n    # Remove common separators\n    cleaned = re.sub(r'[\\s\\-\\(\\)\\.]', '', phone)\n    \n    # Check if it's all digits (possibly with + prefix)\n    if re.match(r'^\\+?\\d{10,15}{
  "problems": [
    {
      "id": 129,
      "title": "CSV File Parser and Data Extractor",
      "statement": "Parse a CSV file content and extract specific columns with data validation. Handle various delimiters (comma, semicolon, tab), quoted fields, and missing values. Return structured data with column headers and rows. For example, parse 'Name,Age,City\\nJohn,25,NYC\\nJane,30,LA' and extract specific columns with type conversion.",
      "function_signature": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):",
      "categories": ["input_output_processing", "data_parsing"],
      "python_constructs": ["csv_parsing", "string_manipulation", "data_validation"],
      "hints": [
        "Use csv module or manual parsing to handle different delimiters",
        "Handle quoted fields that may contain delimiter characters",
        "Implement type conversion for numeric columns and date parsing"
      ],
      "solution": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):\n    import csv\n    from io import StringIO\n    \n    if not csv_content:\n        return {'headers': [], 'rows': [], 'error': None}\n    \n    try:\n        # Parse CSV content\n        csv_reader = csv.reader(StringIO(csv_content), delimiter=delimiter)\n        rows = list(csv_reader)\n        \n        if not rows:\n            return {'headers': [], 'rows': [], 'error': None}\n        \n        headers = rows[0]\n        data_rows = rows[1:]\n        \n        # Extract specific columns if requested\n        if extract_columns:\n            try:\n                # Find column indices\n                column_indices = []\n                for col in extract_columns:\n                    if isinstance(col, str):\n                        if col in headers:\n                            column_indices.append(headers.index(col))\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column \"{col}\" not found'}\n                    elif isinstance(col, int):\n                        if 0 <= col < len(headers):\n                            column_indices.append(col)\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column index {col} out of range'}\n                \n                # Extract columns\n                filtered_headers = [headers[i] for i in column_indices]\n                filtered_rows = []\n                \n                for row in data_rows:\n                    filtered_row = []\n                    for i in column_indices:\n                        if i < len(row):\n                            filtered_row.append(row[i])\n                        else:\n                            filtered_row.append('')  # Missing value\n                    filtered_rows.append(filtered_row)\n                \n                headers = filtered_headers\n                data_rows = filtered_rows\n            \n            except Exception as e:\n                return {'headers': [], 'rows': [], 'error': f'Column extraction error: {str(e)}'}\n        \n        # Apply type conversions\n        if convert_types:\n            converted_rows = []\n            for row in data_rows:\n                converted_row = []\n                for i, value in enumerate(row):\n                    if i < len(headers) and headers[i] in convert_types:\n                        try:\n                            converter = convert_types[headers[i]]\n                            if converter == 'int':\n                                converted_row.append(int(value) if value.strip() else None)\n                            elif converter == 'float':\n                                converted_row.append(float(value) if value.strip() else None)\n                            elif converter == 'bool':\n                                converted_row.append(value.lower() in ['true', '1', 'yes', 'on'])\n                            else:\n                                converted_row.append(value)\n                        except (ValueError, AttributeError):\n                            converted_row.append(None)  # Conversion failed\n                    else:\n                        converted_row.append(value)\n                converted_rows.append(converted_row)\n            data_rows = converted_rows\n        \n        return {\n            'headers': headers,\n            'rows': data_rows,\n            'row_count': len(data_rows),\n            'column_count': len(headers),\n            'error': None\n        }\n    \n    except Exception as e:\n        return {'headers': [], 'rows': [], 'error': f'Parsing error: {str(e)}'}"
      ],
      "test_cases": [
        {"input": ["Name,Age,City\\nJohn,25,NYC\\nJane,30,LA", ",", null, null], "expected": {"headers": ["Name", "Age", "City"], "rows": [["John", "25", "NYC"], ["Jane", "30", "LA"]], "row_count": 2, "column_count": 3, "error": null}},
        {"input": ["Name;Age;Salary\\nJohn;25;50000\\nJane;30;60000", ";", ["Name", "Salary"], {"Salary": "int"}], "expected": {"headers": ["Name", "Salary"], "rows": [["John", 50000], ["Jane", 60000]], "row_count": 2, "column_count": 2, "error": null}},
        {"input": ["A,B,C\\n1,2,3", ",", [0, 2], {"A": "int", "C": "int"}], "expected": {"headers": ["A", "C"], "rows": [[1, 3]], "row_count": 1, "column_count": 2, "error": null}},
        {"input": [""], "expected": {"headers": [], "rows": [], "error": null}},
        {"input": ["Name,Age\\nJohn,25", ",", ["NonExistent"], null], "expected": {"headers": [], "rows": [], "error": "Column \"NonExistent\" not found"}}
      ]
    },
    {
      "id": 130,
      "title": "File Content Word and Line Counter",
      "statement": "Count words, lines, characters, and paragraphs in text content. Provide detailed statistics including average word length, sentence count, and most frequent words. Handle different text encodings and empty lines. Return comprehensive text analysis results.",
      "function_signature": "def analyze_text_content(content, include_word_freq=True, top_words=10):",
      "categories": ["input_output_processing", "text_analysis"],
      "python_constructs": ["text_processing", "regular_expressions", "frequency_analysis"],
      "hints": [
        "Use regular expressions to identify words, sentences, and paragraphs",
        "Handle different definitions of word boundaries and punctuation",
        "Calculate frequency distribution and sort by occurrence count"
      ],
      "solution": "def analyze_text_content(content, include_word_freq=True, top_words=10):\n    import re\n    from collections import Counter\n    \n    if not isinstance(content, str):\n        return None\n    \n    if not content:\n        return {\n            'characters': 0,\n            'characters_no_spaces': 0,\n            'words': 0,\n            'lines': 0,\n            'paragraphs': 0,\n            'sentences': 0,\n            'avg_word_length': 0.0,\n            'word_frequency': {} if include_word_freq else None\n        }\n    \n    # Basic counts\n    char_count = len(content)\n    char_count_no_spaces = len(content.replace(' ', '').replace('\\t', '').replace('\\n', '').replace('\\r', ''))\n    \n    # Line count\n    lines = content.split('\\n')\n    line_count = len(lines)\n    \n    # Paragraph count (separated by empty lines)\n    paragraphs = re.split(r'\\n\\s*\\n', content.strip())\n    paragraph_count = len([p for p in paragraphs if p.strip()])\n    \n    # Word extraction and counting\n    words = re.findall(r\"\\b\\w+\\b\", content.lower())\n    word_count = len(words)\n    \n    # Average word length\n    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0.0\n    \n    # Sentence count (approximate)\n    sentences = re.split(r'[.!?]+', content)\n    sentence_count = len([s for s in sentences if s.strip()])\n    \n    result = {\n        'characters': char_count,\n        'characters_no_spaces': char_count_no_spaces,\n        'words': word_count,\n        'lines': line_count,\n        'paragraphs': paragraph_count,\n        'sentences': sentence_count,\n        'avg_word_length': round(avg_word_length, 2)\n    }\n    \n    # Word frequency analysis\n    if include_word_freq and words:\n        word_freq = Counter(words)\n        top_word_list = word_freq.most_common(top_words)\n        result['word_frequency'] = {\n            'total_unique_words': len(word_freq),\n            'top_words': top_word_list,\n            'most_common_word': top_word_list[0] if top_word_list else None\n        }\n    else:\n        result['word_frequency'] = None\n    \n    return result",
      "test_cases": [
        {"input": ["Hello world! This is a test.\\nAnother line here.", true, 5], "expected": {"characters": 46, "characters_no_spaces": 37, "words": 9, "lines": 2, "paragraphs": 1, "sentences": 2, "avg_word_length": 4.0, "word_frequency": {"total_unique_words": 9, "top_words": [["hello", 1], ["world", 1], ["this", 1], ["is", 1], ["a", 1]], "most_common_word": ["hello", 1]}}},
        {"input": [""], "expected": {"characters": 0, "characters_no_spaces": 0, "words": 0, "lines": 0, "paragraphs": 0, "sentences": 0, "avg_word_length": 0.0, "word_frequency": {}}},
        {"input": ["Word word WORD!", false, 3], "expected": {"characters": 14, "characters_no_spaces": 11, "words": 3, "lines": 1, "paragraphs": 1, "sentences": 1, "avg_word_length": 4.0, "word_frequency": null}},
        {"input": ["Line 1\\n\\nParagraph 2\\nStill paragraph 2", true, 3], "expected": {"characters": 35, "characters_no_spaces": 25, "words": 6, "lines": 4, "paragraphs": 2, "sentences": 1, "avg_word_length": 4.17, "word_frequency": {"total_unique_words": 6, "top_words": [["line", 1], ["1", 1], ["paragraph", 2]], "most_common_word": ["paragraph", 2]}}},
        {"input": ["Hello! How are you? I am fine.", true, 10], "expected": {"characters": 30, "characters_no_spaces": 24, "words": 7, "lines": 1, "paragraphs": 1, "sentences": 3, "avg_word_length": 3.0, "word_frequency": {"total_unique_words": 7, "top_words": [["hello", 1], ["how", 1], ["are", 1], ["you", 1], ["i", 1], ["am", 1], ["fine", 1]], "most_common_word": ["hello", 1]}}}
      ]
    },
    {
      "id": 131,
      "title": "JSON Data Parser and Validator",
      "statement": "Parse and validate JSON data with schema checking, nested object extraction, and data transformation. Handle malformed JSON, validate data types, and extract specific fields using JSONPath-like syntax. Return parsed data with validation results and extracted fields.",
      "function_signature": "def parse_json_data(json_string, schema=None, extract_paths=None):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["json_parsing", "data_validation", "nested_data_access"],
      "hints": [
        "Use json module for parsing and handle JSONDecodeError exceptions",
        "Implement schema validation for required fields and data types",
        "Support dot notation for nested field extraction"
      ],
      "solution": "def parse_json_data(json_string, schema=None, extract_paths=None):\n    import json\n    \n    result = {\n        'valid': False,\n        'data': None,\n        'errors': [],\n        'extracted_fields': {},\n        'schema_valid': True\n    }\n    \n    # Parse JSON\n    try:\n        data = json.loads(json_string)\n        result['valid'] = True\n        result['data'] = data\n    except json.JSONDecodeError as e:\n        result['errors'].append(f'JSON parsing error: {str(e)}')\n        return result\n    except Exception as e:\n        result['errors'].append(f'Unexpected error: {str(e)}')\n        return result\n    \n    # Schema validation\n    if schema:\n        schema_errors = validate_schema(data, schema)\n        if schema_errors:\n            result['schema_valid'] = False\n            result['errors'].extend(schema_errors)\n    \n    # Extract specific fields\n    if extract_paths:\n        for path_name, path in extract_paths.items():\n            try:\n                extracted_value = extract_nested_value(data, path)\n                result['extracted_fields'][path_name] = extracted_value\n            except Exception as e:\n                result['errors'].append(f'Extraction error for \"{path_name}\": {str(e)}')\n                result['extracted_fields'][path_name] = None\n    \n    return result\n\ndef validate_schema(data, schema):\n    errors = []\n    \n    # Check required fields\n    if 'required' in schema:\n        for field in schema['required']:\n            if field not in data:\n                errors.append(f'Missing required field: {field}')\n    \n    # Check field types\n    if 'properties' in schema:\n        for field, field_schema in schema['properties'].items():\n            if field in data:\n                expected_type = field_schema.get('type')\n                actual_value = data[field]\n                \n                if expected_type == 'string' and not isinstance(actual_value, str):\n                    errors.append(f'Field \"{field}\" should be string, got {type(actual_value).__name__}')\n                elif expected_type == 'number' and not isinstance(actual_value, (int, float)):\n                    errors.append(f'Field \"{field}\" should be number, got {type(actual_value).__name__}')\n                elif expected_type == 'integer' and not isinstance(actual_value, int):\n                    errors.append(f'Field \"{field}\" should be integer, got {type(actual_value).__name__}')\n                elif expected_type == 'boolean' and not isinstance(actual_value, bool):\n                    errors.append(f'Field \"{field}\" should be boolean, got {type(actual_value).__name__}')\n                elif expected_type == 'array' and not isinstance(actual_value, list):\n                    errors.append(f'Field \"{field}\" should be array, got {type(actual_value).__name__}')\n                elif expected_type == 'object' and not isinstance(actual_value, dict):\n                    errors.append(f'Field \"{field}\" should be object, got {type(actual_value).__name__}')\n    \n    return errors\n\ndef extract_nested_value(data, path):\n    \"\"\"Extract value using dot notation path like 'user.profile.name'\"\"\"\n    keys = path.split('.')\n    current = data\n    \n    for key in keys:\n        if isinstance(current, dict) and key in current:\n            current = current[key]\n        elif isinstance(current, list) and key.isdigit():\n            index = int(key)\n            if 0 <= index < len(current):\n                current = current[index]\n            else:\n                raise KeyError(f'List index {index} out of range')\n        else:\n            raise KeyError(f'Key \"{key}\" not found in path \"{path}\"')\n    \n    return current",
      "test_cases": [
        {"input": ["{\"name\": \"John\", \"age\": 30, \"city\": \"NYC\"}", null, null], "expected": {"valid": true, "data": {"name": "John", "age": 30, "city": "NYC"}, "errors": [], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": \"John\", \"age\": 30}", {"required": ["name", "age"], "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}, {"user_name": "name", "user_age": "age"}], "expected": {"valid": true, "data": {"name": "John", "age": 30}, "errors": [], "extracted_fields": {"user_name": "John", "user_age": 30}, "schema_valid": true}},
        {"input": ["{\"user\": {\"profile\": {\"name\": \"Alice\"}}}", null, {"nested_name": "user.profile.name"}], "expected": {"valid": true, "data": {"user": {"profile": {"name": "Alice"}}}, "errors": [], "extracted_fields": {"nested_name": "Alice"}, "schema_valid": true}},
        {"input": ["invalid json", null, null], "expected": {"valid": false, "data": null, "errors": ["JSON parsing error: Expecting value: line 1 column 1 (char 0)"], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": 123}", {"required": ["name"], "properties": {"name": {"type": "string"}}}, null], "expected": {"valid": true, "data": {"name": 123}, "errors": ["Field \"name\" should be string, got int"], "extracted_fields": {}, "schema_valid": false}}
      ]
    },
    {
      "id": 132,
      "title": "Command Line Arguments Processor",
      "statement": "Process command line arguments with support for flags, options with values, positional arguments, and help generation. Handle short (-h) and long (--help) options, validate required arguments, and provide default values. Return parsed arguments dictionary with validation results.",
      "function_signature": "def process_command_args(args_list, arg_definitions):",
      "categories": ["input_output_processing", "argument_parsing"],
      "python_constructs": ["argument_parsing", "command_line_interface", "validation_logic"],
      "hints": [
        "Parse arguments into flags (boolean), options (key-value), and positional arguments",
        "Support both short (-h) and long (--help) argument formats",
        "Validate required arguments and apply default values"
      ],
      "solution": "def process_command_args(args_list, arg_definitions):\n    result = {\n        'parsed_args': {},\n        'positional_args': [],\n        'errors': [],\n        'help_requested': False\n    }\n    \n    if not args_list:\n        args_list = []\n    \n    # Check for help request\n    if '-h' in args_list or '--help' in args_list:\n        result['help_requested'] = True\n        result['help_text'] = generate_help_text(arg_definitions)\n        return result\n    \n    # Initialize with default values\n    for arg_name, arg_config in arg_definitions.items():\n        if 'default' in arg_config:\n            result['parsed_args'][arg_name] = arg_config['default']\n    \n    i = 0\n    while i < len(args_list):\n        arg = args_list[i]\n        \n        if arg.startswith('--'):\n            # Long option\n            option_name = arg[2:]\n            arg_config = find_arg_config(option_name, arg_definitions, 'long')\n            \n            if not arg_config:\n                result['errors'].append(f'Unknown option: {arg}')\n                i += 1\n                continue\n            \n            arg_name, config = arg_config\n            \n            if config.get('type') == 'flag':\n                result['parsed_args'][arg_name] = True\n                i += 1\n            else:\n                # Option with value\n                if i + 1 >= len(args_list):\n                    result['errors'].append(f'Option {arg} requires a value')\n                    i += 1\n                    continue\n                \n                value = args_list[i + 1]\n                converted_value = convert_arg_value(value, config.get('value_type', 'string'))\n                \n                if converted_value is None and config.get('value_type') != 'string':\n                    result['errors'].append(f'Invalid {config.get(\"value_type\")} value for {arg}: {value}')\n                else:\n                    result['parsed_args'][arg_name] = converted_value if converted_value is not None else value\n                \n                i += 2\n        \n        elif arg.startswith('-') and len(arg) > 1:\n            # Short option(s)\n            for char in arg[1:]:\n                arg_config = find_arg_config(char, arg_definitions, 'short')\n                \n                if not arg_config:\n                    result['errors'].append(f'Unknown option: -{char}')\n                    continue\n                \n                arg_name, config = arg_config\n                \n                if config.get('type') == 'flag':\n                    result['parsed_args'][arg_name] = True\n            i += 1\n        \n        else:\n            # Positional argument\n            result['positional_args'].append(arg)\n            i += 1\n    \n    # Validate required arguments\n    for arg_name, arg_config in arg_definitions.items():\n        if arg_config.get('required', False) and arg_name not in result['parsed_args']:\n            result['errors'].append(f'Required argument missing: {arg_name}')\n    \n    return result\n\ndef find_arg_config(option, arg_definitions, option_type):\n    for arg_name, config in arg_definitions.items():\n        if option_type == 'long' and config.get('long') == option:\n            return (arg_name, config)\n        elif option_type == 'short' and config.get('short') == option:\n            return (arg_name, config)\n    return None\n\ndef convert_arg_value(value, value_type):\n    try:\n        if value_type == 'int':\n            return int(value)\n        elif value_type == 'float':\n            return float(value)\n        elif value_type == 'bool':\n            return value.lower() in ['true', '1', 'yes', 'on']\n        else:\n            return value\n    except (ValueError, AttributeError):\n        return None\n\ndef generate_help_text(arg_definitions):\n    help_lines = ['Available options:']\n    \n    for arg_name, config in arg_definitions.items():\n        line_parts = []\n        \n        if config.get('short'):\n            line_parts.append(f'-{config[\"short\"]}')\n        \n        if config.get('long'):\n            line_parts.append(f'--{config[\"long\"]}')\n        \n        option_text = ', '.join(line_parts)\n        \n        if config.get('description'):\n            option_text += f': {config[\"description\"]}'\n        \n        if config.get('required'):\n            option_text += ' (required)'\n        \n        if 'default' in config:\n            option_text += f' (default: {config[\"default\"]})')\n        \n        help_lines.append(f'  {option_text}')\n    \n    return '\\n'.join(help_lines)",
      "test_cases": [
        {"input": [["--input", "file.txt", "-v", "--output", "result.txt"], {"input_file": {"long": "input", "required": true}, "verbose": {"short": "v", "type": "flag"}, "output_file": {"long": "output", "default": "output.txt"}}], "expected": {"parsed_args": {"input_file": "file.txt", "verbose": true, "output_file": "result.txt"}, "positional_args": [], "errors": [], "help_requested": false}},
        {"input": [["-h"], {"verbose": {"short": "v", "type": "flag"}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": [], "help_requested": true, "help_text": "Available options:\\n  -v: (required)"}},
        {"input": [["--count", "5", "file1", "file2"], {"count": {"long": "count", "value_type": "int", "default": 1}}], "expected": {"parsed_args": {"count": 5}, "positional_args": ["file1", "file2"], "errors": [], "help_requested": false}},
        {"input": [["--missing"], {"required_arg": {"long": "required", "required": true}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": ["Unknown option: --missing", "Required argument missing: required_arg"], "help_requested": false}},
        {"input": [[], {"default_val": {"long": "default", "default": "test"}}], "expected": {"parsed_args": {"default_val": "test"}, "positional_args": [], "errors": [], "help_requested": false}}
      ]
    },
    {
      "id": 133,
      "title": "Data Validation Engine",
      "statement": "Validate various data types including email addresses, phone numbers, credit card numbers, URLs, and custom patterns. Return detailed validation results with specific error messages and suggestions for correction. Support multiple validation rules per field.",
      "function_signature": "def validate_data_fields(data, validation_rules):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["regular_expressions", "validation_patterns", "error_handling"],
      "hints": [
        "Use regular expressions for pattern matching (email, phone, URL)",
        "Implement Luhn algorithm for credit card validation",
        "Provide specific error messages and correction suggestions"
      ],
      "solution": "def validate_data_fields(data, validation_rules):\n    import re\n    \n    result = {\n        'valid': True,\n        'field_results': {},\n        'errors': [],\n        'warnings': []\n    }\n    \n    for field_name, rules in validation_rules.items():\n        field_result = {\n            'valid': True,\n            'value': data.get(field_name),\n            'errors': [],\n            'warnings': []\n        }\n        \n        field_value = data.get(field_name)\n        \n        # Check if field is required\n        if rules.get('required', False) and (field_value is None or field_value == ''):\n            field_result['valid'] = False\n            field_result['errors'].append('Field is required')\n            result['valid'] = False\n        \n        # Skip further validation if field is empty and not required\n        if field_value is None or field_value == '':\n            result['field_results'][field_name] = field_result\n            continue\n        \n        # Validate based on data type\n        data_type = rules.get('type', 'string')\n        \n        if data_type == 'email':\n            if not validate_email(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid email format')\n                result['valid'] = False\n        \n        elif data_type == 'phone':\n            phone_result = validate_phone(field_value)\n            if not phone_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(phone_result['error'])\n                result['valid'] = False\n        \n        elif data_type == 'credit_card':\n            cc_result = validate_credit_card(field_value)\n            if not cc_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(cc_result['error'])\n                result['valid'] = False\n            else:\n                field_result['card_type'] = cc_result.get('card_type')\n        \n        elif data_type == 'url':\n            if not validate_url(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid URL format')\n                result['valid'] = False\n        \n        elif data_type == 'pattern':\n            pattern = rules.get('pattern')\n            if pattern and not re.match(pattern, str(field_value)):\n                field_result['valid'] = False\n                field_result['errors'].append(f'Does not match required pattern: {pattern}')\n                result['valid'] = False\n        \n        # Length validation\n        if 'min_length' in rules:\n            if len(str(field_value)) < rules['min_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Minimum length is {rules[\"min_length\"]}')\n                result['valid'] = False\n        \n        if 'max_length' in rules:\n            if len(str(field_value)) > rules['max_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Maximum length is {rules[\"max_length\"]}')\n                result['valid'] = False\n        \n        # Range validation for numbers\n        if data_type in ['int', 'float']:\n            try:\n                num_value = float(field_value)\n                if 'min_value' in rules and num_value < rules['min_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Minimum value is {rules[\"min_value\"]}')\n                    result['valid'] = False\n                \n                if 'max_value' in rules and num_value > rules['max_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Maximum value is {rules[\"max_value\"]}')\n                    result['valid'] = False\n            except ValueError:\n                field_result['valid'] =, cleaned):\n        return {'valid': True}\n    else:\n        return {'valid': False, 'error': 'Phone number must be 10-15 digits'}\n\ndef validate_credit_card(card_number):\n    # Remove spaces and dashes\n    cleaned = re.sub(r'[\\s\\-]', '', card_number)\n    \n    # Check if all digits\n    if not re.match(r'^\\d+{
  "problems": [
    {
      "id": 129,
      "title": "CSV File Parser and Data Extractor",
      "statement": "Parse a CSV file content and extract specific columns with data validation. Handle various delimiters (comma, semicolon, tab), quoted fields, and missing values. Return structured data with column headers and rows. For example, parse 'Name,Age,City\\nJohn,25,NYC\\nJane,30,LA' and extract specific columns with type conversion.",
      "function_signature": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):",
      "categories": ["input_output_processing", "data_parsing"],
      "python_constructs": ["csv_parsing", "string_manipulation", "data_validation"],
      "hints": [
        "Use csv module or manual parsing to handle different delimiters",
        "Handle quoted fields that may contain delimiter characters",
        "Implement type conversion for numeric columns and date parsing"
      ],
      "solution": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):\n    import csv\n    from io import StringIO\n    \n    if not csv_content:\n        return {'headers': [], 'rows': [], 'error': None}\n    \n    try:\n        # Parse CSV content\n        csv_reader = csv.reader(StringIO(csv_content), delimiter=delimiter)\n        rows = list(csv_reader)\n        \n        if not rows:\n            return {'headers': [], 'rows': [], 'error': None}\n        \n        headers = rows[0]\n        data_rows = rows[1:]\n        \n        # Extract specific columns if requested\n        if extract_columns:\n            try:\n                # Find column indices\n                column_indices = []\n                for col in extract_columns:\n                    if isinstance(col, str):\n                        if col in headers:\n                            column_indices.append(headers.index(col))\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column \"{col}\" not found'}\n                    elif isinstance(col, int):\n                        if 0 <= col < len(headers):\n                            column_indices.append(col)\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column index {col} out of range'}\n                \n                # Extract columns\n                filtered_headers = [headers[i] for i in column_indices]\n                filtered_rows = []\n                \n                for row in data_rows:\n                    filtered_row = []\n                    for i in column_indices:\n                        if i < len(row):\n                            filtered_row.append(row[i])\n                        else:\n                            filtered_row.append('')  # Missing value\n                    filtered_rows.append(filtered_row)\n                \n                headers = filtered_headers\n                data_rows = filtered_rows\n            \n            except Exception as e:\n                return {'headers': [], 'rows': [], 'error': f'Column extraction error: {str(e)}'}\n        \n        # Apply type conversions\n        if convert_types:\n            converted_rows = []\n            for row in data_rows:\n                converted_row = []\n                for i, value in enumerate(row):\n                    if i < len(headers) and headers[i] in convert_types:\n                        try:\n                            converter = convert_types[headers[i]]\n                            if converter == 'int':\n                                converted_row.append(int(value) if value.strip() else None)\n                            elif converter == 'float':\n                                converted_row.append(float(value) if value.strip() else None)\n                            elif converter == 'bool':\n                                converted_row.append(value.lower() in ['true', '1', 'yes', 'on'])\n                            else:\n                                converted_row.append(value)\n                        except (ValueError, AttributeError):\n                            converted_row.append(None)  # Conversion failed\n                    else:\n                        converted_row.append(value)\n                converted_rows.append(converted_row)\n            data_rows = converted_rows\n        \n        return {\n            'headers': headers,\n            'rows': data_rows,\n            'row_count': len(data_rows),\n            'column_count': len(headers),\n            'error': None\n        }\n    \n    except Exception as e:\n        return {'headers': [], 'rows': [], 'error': f'Parsing error: {str(e)}'}"
      ],
      "test_cases": [
        {"input": ["Name,Age,City\\nJohn,25,NYC\\nJane,30,LA", ",", null, null], "expected": {"headers": ["Name", "Age", "City"], "rows": [["John", "25", "NYC"], ["Jane", "30", "LA"]], "row_count": 2, "column_count": 3, "error": null}},
        {"input": ["Name;Age;Salary\\nJohn;25;50000\\nJane;30;60000", ";", ["Name", "Salary"], {"Salary": "int"}], "expected": {"headers": ["Name", "Salary"], "rows": [["John", 50000], ["Jane", 60000]], "row_count": 2, "column_count": 2, "error": null}},
        {"input": ["A,B,C\\n1,2,3", ",", [0, 2], {"A": "int", "C": "int"}], "expected": {"headers": ["A", "C"], "rows": [[1, 3]], "row_count": 1, "column_count": 2, "error": null}},
        {"input": [""], "expected": {"headers": [], "rows": [], "error": null}},
        {"input": ["Name,Age\\nJohn,25", ",", ["NonExistent"], null], "expected": {"headers": [], "rows": [], "error": "Column \"NonExistent\" not found"}}
      ]
    },
    {
      "id": 130,
      "title": "File Content Word and Line Counter",
      "statement": "Count words, lines, characters, and paragraphs in text content. Provide detailed statistics including average word length, sentence count, and most frequent words. Handle different text encodings and empty lines. Return comprehensive text analysis results.",
      "function_signature": "def analyze_text_content(content, include_word_freq=True, top_words=10):",
      "categories": ["input_output_processing", "text_analysis"],
      "python_constructs": ["text_processing", "regular_expressions", "frequency_analysis"],
      "hints": [
        "Use regular expressions to identify words, sentences, and paragraphs",
        "Handle different definitions of word boundaries and punctuation",
        "Calculate frequency distribution and sort by occurrence count"
      ],
      "solution": "def analyze_text_content(content, include_word_freq=True, top_words=10):\n    import re\n    from collections import Counter\n    \n    if not isinstance(content, str):\n        return None\n    \n    if not content:\n        return {\n            'characters': 0,\n            'characters_no_spaces': 0,\n            'words': 0,\n            'lines': 0,\n            'paragraphs': 0,\n            'sentences': 0,\n            'avg_word_length': 0.0,\n            'word_frequency': {} if include_word_freq else None\n        }\n    \n    # Basic counts\n    char_count = len(content)\n    char_count_no_spaces = len(content.replace(' ', '').replace('\\t', '').replace('\\n', '').replace('\\r', ''))\n    \n    # Line count\n    lines = content.split('\\n')\n    line_count = len(lines)\n    \n    # Paragraph count (separated by empty lines)\n    paragraphs = re.split(r'\\n\\s*\\n', content.strip())\n    paragraph_count = len([p for p in paragraphs if p.strip()])\n    \n    # Word extraction and counting\n    words = re.findall(r\"\\b\\w+\\b\", content.lower())\n    word_count = len(words)\n    \n    # Average word length\n    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0.0\n    \n    # Sentence count (approximate)\n    sentences = re.split(r'[.!?]+', content)\n    sentence_count = len([s for s in sentences if s.strip()])\n    \n    result = {\n        'characters': char_count,\n        'characters_no_spaces': char_count_no_spaces,\n        'words': word_count,\n        'lines': line_count,\n        'paragraphs': paragraph_count,\n        'sentences': sentence_count,\n        'avg_word_length': round(avg_word_length, 2)\n    }\n    \n    # Word frequency analysis\n    if include_word_freq and words:\n        word_freq = Counter(words)\n        top_word_list = word_freq.most_common(top_words)\n        result['word_frequency'] = {\n            'total_unique_words': len(word_freq),\n            'top_words': top_word_list,\n            'most_common_word': top_word_list[0] if top_word_list else None\n        }\n    else:\n        result['word_frequency'] = None\n    \n    return result",
      "test_cases": [
        {"input": ["Hello world! This is a test.\\nAnother line here.", true, 5], "expected": {"characters": 46, "characters_no_spaces": 37, "words": 9, "lines": 2, "paragraphs": 1, "sentences": 2, "avg_word_length": 4.0, "word_frequency": {"total_unique_words": 9, "top_words": [["hello", 1], ["world", 1], ["this", 1], ["is", 1], ["a", 1]], "most_common_word": ["hello", 1]}}},
        {"input": [""], "expected": {"characters": 0, "characters_no_spaces": 0, "words": 0, "lines": 0, "paragraphs": 0, "sentences": 0, "avg_word_length": 0.0, "word_frequency": {}}},
        {"input": ["Word word WORD!", false, 3], "expected": {"characters": 14, "characters_no_spaces": 11, "words": 3, "lines": 1, "paragraphs": 1, "sentences": 1, "avg_word_length": 4.0, "word_frequency": null}},
        {"input": ["Line 1\\n\\nParagraph 2\\nStill paragraph 2", true, 3], "expected": {"characters": 35, "characters_no_spaces": 25, "words": 6, "lines": 4, "paragraphs": 2, "sentences": 1, "avg_word_length": 4.17, "word_frequency": {"total_unique_words": 6, "top_words": [["line", 1], ["1", 1], ["paragraph", 2]], "most_common_word": ["paragraph", 2]}}},
        {"input": ["Hello! How are you? I am fine.", true, 10], "expected": {"characters": 30, "characters_no_spaces": 24, "words": 7, "lines": 1, "paragraphs": 1, "sentences": 3, "avg_word_length": 3.0, "word_frequency": {"total_unique_words": 7, "top_words": [["hello", 1], ["how", 1], ["are", 1], ["you", 1], ["i", 1], ["am", 1], ["fine", 1]], "most_common_word": ["hello", 1]}}}
      ]
    },
    {
      "id": 131,
      "title": "JSON Data Parser and Validator",
      "statement": "Parse and validate JSON data with schema checking, nested object extraction, and data transformation. Handle malformed JSON, validate data types, and extract specific fields using JSONPath-like syntax. Return parsed data with validation results and extracted fields.",
      "function_signature": "def parse_json_data(json_string, schema=None, extract_paths=None):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["json_parsing", "data_validation", "nested_data_access"],
      "hints": [
        "Use json module for parsing and handle JSONDecodeError exceptions",
        "Implement schema validation for required fields and data types",
        "Support dot notation for nested field extraction"
      ],
      "solution": "def parse_json_data(json_string, schema=None, extract_paths=None):\n    import json\n    \n    result = {\n        'valid': False,\n        'data': None,\n        'errors': [],\n        'extracted_fields': {},\n        'schema_valid': True\n    }\n    \n    # Parse JSON\n    try:\n        data = json.loads(json_string)\n        result['valid'] = True\n        result['data'] = data\n    except json.JSONDecodeError as e:\n        result['errors'].append(f'JSON parsing error: {str(e)}')\n        return result\n    except Exception as e:\n        result['errors'].append(f'Unexpected error: {str(e)}')\n        return result\n    \n    # Schema validation\n    if schema:\n        schema_errors = validate_schema(data, schema)\n        if schema_errors:\n            result['schema_valid'] = False\n            result['errors'].extend(schema_errors)\n    \n    # Extract specific fields\n    if extract_paths:\n        for path_name, path in extract_paths.items():\n            try:\n                extracted_value = extract_nested_value(data, path)\n                result['extracted_fields'][path_name] = extracted_value\n            except Exception as e:\n                result['errors'].append(f'Extraction error for \"{path_name}\": {str(e)}')\n                result['extracted_fields'][path_name] = None\n    \n    return result\n\ndef validate_schema(data, schema):\n    errors = []\n    \n    # Check required fields\n    if 'required' in schema:\n        for field in schema['required']:\n            if field not in data:\n                errors.append(f'Missing required field: {field}')\n    \n    # Check field types\n    if 'properties' in schema:\n        for field, field_schema in schema['properties'].items():\n            if field in data:\n                expected_type = field_schema.get('type')\n                actual_value = data[field]\n                \n                if expected_type == 'string' and not isinstance(actual_value, str):\n                    errors.append(f'Field \"{field}\" should be string, got {type(actual_value).__name__}')\n                elif expected_type == 'number' and not isinstance(actual_value, (int, float)):\n                    errors.append(f'Field \"{field}\" should be number, got {type(actual_value).__name__}')\n                elif expected_type == 'integer' and not isinstance(actual_value, int):\n                    errors.append(f'Field \"{field}\" should be integer, got {type(actual_value).__name__}')\n                elif expected_type == 'boolean' and not isinstance(actual_value, bool):\n                    errors.append(f'Field \"{field}\" should be boolean, got {type(actual_value).__name__}')\n                elif expected_type == 'array' and not isinstance(actual_value, list):\n                    errors.append(f'Field \"{field}\" should be array, got {type(actual_value).__name__}')\n                elif expected_type == 'object' and not isinstance(actual_value, dict):\n                    errors.append(f'Field \"{field}\" should be object, got {type(actual_value).__name__}')\n    \n    return errors\n\ndef extract_nested_value(data, path):\n    \"\"\"Extract value using dot notation path like 'user.profile.name'\"\"\"\n    keys = path.split('.')\n    current = data\n    \n    for key in keys:\n        if isinstance(current, dict) and key in current:\n            current = current[key]\n        elif isinstance(current, list) and key.isdigit():\n            index = int(key)\n            if 0 <= index < len(current):\n                current = current[index]\n            else:\n                raise KeyError(f'List index {index} out of range')\n        else:\n            raise KeyError(f'Key \"{key}\" not found in path \"{path}\"')\n    \n    return current",
      "test_cases": [
        {"input": ["{\"name\": \"John\", \"age\": 30, \"city\": \"NYC\"}", null, null], "expected": {"valid": true, "data": {"name": "John", "age": 30, "city": "NYC"}, "errors": [], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": \"John\", \"age\": 30}", {"required": ["name", "age"], "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}, {"user_name": "name", "user_age": "age"}], "expected": {"valid": true, "data": {"name": "John", "age": 30}, "errors": [], "extracted_fields": {"user_name": "John", "user_age": 30}, "schema_valid": true}},
        {"input": ["{\"user\": {\"profile\": {\"name\": \"Alice\"}}}", null, {"nested_name": "user.profile.name"}], "expected": {"valid": true, "data": {"user": {"profile": {"name": "Alice"}}}, "errors": [], "extracted_fields": {"nested_name": "Alice"}, "schema_valid": true}},
        {"input": ["invalid json", null, null], "expected": {"valid": false, "data": null, "errors": ["JSON parsing error: Expecting value: line 1 column 1 (char 0)"], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": 123}", {"required": ["name"], "properties": {"name": {"type": "string"}}}, null], "expected": {"valid": true, "data": {"name": 123}, "errors": ["Field \"name\" should be string, got int"], "extracted_fields": {}, "schema_valid": false}}
      ]
    },
    {
      "id": 132,
      "title": "Command Line Arguments Processor",
      "statement": "Process command line arguments with support for flags, options with values, positional arguments, and help generation. Handle short (-h) and long (--help) options, validate required arguments, and provide default values. Return parsed arguments dictionary with validation results.",
      "function_signature": "def process_command_args(args_list, arg_definitions):",
      "categories": ["input_output_processing", "argument_parsing"],
      "python_constructs": ["argument_parsing", "command_line_interface", "validation_logic"],
      "hints": [
        "Parse arguments into flags (boolean), options (key-value), and positional arguments",
        "Support both short (-h) and long (--help) argument formats",
        "Validate required arguments and apply default values"
      ],
      "solution": "def process_command_args(args_list, arg_definitions):\n    result = {\n        'parsed_args': {},\n        'positional_args': [],\n        'errors': [],\n        'help_requested': False\n    }\n    \n    if not args_list:\n        args_list = []\n    \n    # Check for help request\n    if '-h' in args_list or '--help' in args_list:\n        result['help_requested'] = True\n        result['help_text'] = generate_help_text(arg_definitions)\n        return result\n    \n    # Initialize with default values\n    for arg_name, arg_config in arg_definitions.items():\n        if 'default' in arg_config:\n            result['parsed_args'][arg_name] = arg_config['default']\n    \n    i = 0\n    while i < len(args_list):\n        arg = args_list[i]\n        \n        if arg.startswith('--'):\n            # Long option\n            option_name = arg[2:]\n            arg_config = find_arg_config(option_name, arg_definitions, 'long')\n            \n            if not arg_config:\n                result['errors'].append(f'Unknown option: {arg}')\n                i += 1\n                continue\n            \n            arg_name, config = arg_config\n            \n            if config.get('type') == 'flag':\n                result['parsed_args'][arg_name] = True\n                i += 1\n            else:\n                # Option with value\n                if i + 1 >= len(args_list):\n                    result['errors'].append(f'Option {arg} requires a value')\n                    i += 1\n                    continue\n                \n                value = args_list[i + 1]\n                converted_value = convert_arg_value(value, config.get('value_type', 'string'))\n                \n                if converted_value is None and config.get('value_type') != 'string':\n                    result['errors'].append(f'Invalid {config.get(\"value_type\")} value for {arg}: {value}')\n                else:\n                    result['parsed_args'][arg_name] = converted_value if converted_value is not None else value\n                \n                i += 2\n        \n        elif arg.startswith('-') and len(arg) > 1:\n            # Short option(s)\n            for char in arg[1:]:\n                arg_config = find_arg_config(char, arg_definitions, 'short')\n                \n                if not arg_config:\n                    result['errors'].append(f'Unknown option: -{char}')\n                    continue\n                \n                arg_name, config = arg_config\n                \n                if config.get('type') == 'flag':\n                    result['parsed_args'][arg_name] = True\n            i += 1\n        \n        else:\n            # Positional argument\n            result['positional_args'].append(arg)\n            i += 1\n    \n    # Validate required arguments\n    for arg_name, arg_config in arg_definitions.items():\n        if arg_config.get('required', False) and arg_name not in result['parsed_args']:\n            result['errors'].append(f'Required argument missing: {arg_name}')\n    \n    return result\n\ndef find_arg_config(option, arg_definitions, option_type):\n    for arg_name, config in arg_definitions.items():\n        if option_type == 'long' and config.get('long') == option:\n            return (arg_name, config)\n        elif option_type == 'short' and config.get('short') == option:\n            return (arg_name, config)\n    return None\n\ndef convert_arg_value(value, value_type):\n    try:\n        if value_type == 'int':\n            return int(value)\n        elif value_type == 'float':\n            return float(value)\n        elif value_type == 'bool':\n            return value.lower() in ['true', '1', 'yes', 'on']\n        else:\n            return value\n    except (ValueError, AttributeError):\n        return None\n\ndef generate_help_text(arg_definitions):\n    help_lines = ['Available options:']\n    \n    for arg_name, config in arg_definitions.items():\n        line_parts = []\n        \n        if config.get('short'):\n            line_parts.append(f'-{config[\"short\"]}')\n        \n        if config.get('long'):\n            line_parts.append(f'--{config[\"long\"]}')\n        \n        option_text = ', '.join(line_parts)\n        \n        if config.get('description'):\n            option_text += f': {config[\"description\"]}'\n        \n        if config.get('required'):\n            option_text += ' (required)'\n        \n        if 'default' in config:\n            option_text += f' (default: {config[\"default\"]})')\n        \n        help_lines.append(f'  {option_text}')\n    \n    return '\\n'.join(help_lines)",
      "test_cases": [
        {"input": [["--input", "file.txt", "-v", "--output", "result.txt"], {"input_file": {"long": "input", "required": true}, "verbose": {"short": "v", "type": "flag"}, "output_file": {"long": "output", "default": "output.txt"}}], "expected": {"parsed_args": {"input_file": "file.txt", "verbose": true, "output_file": "result.txt"}, "positional_args": [], "errors": [], "help_requested": false}},
        {"input": [["-h"], {"verbose": {"short": "v", "type": "flag"}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": [], "help_requested": true, "help_text": "Available options:\\n  -v: (required)"}},
        {"input": [["--count", "5", "file1", "file2"], {"count": {"long": "count", "value_type": "int", "default": 1}}], "expected": {"parsed_args": {"count": 5}, "positional_args": ["file1", "file2"], "errors": [], "help_requested": false}},
        {"input": [["--missing"], {"required_arg": {"long": "required", "required": true}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": ["Unknown option: --missing", "Required argument missing: required_arg"], "help_requested": false}},
        {"input": [[], {"default_val": {"long": "default", "default": "test"}}], "expected": {"parsed_args": {"default_val": "test"}, "positional_args": [], "errors": [], "help_requested": false}}
      ]
    },
    {
      "id": 133,
      "title": "Data Validation Engine",
      "statement": "Validate various data types including email addresses, phone numbers, credit card numbers, URLs, and custom patterns. Return detailed validation results with specific error messages and suggestions for correction. Support multiple validation rules per field.",
      "function_signature": "def validate_data_fields(data, validation_rules):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["regular_expressions", "validation_patterns", "error_handling"],
      "hints": [
        "Use regular expressions for pattern matching (email, phone, URL)",
        "Implement Luhn algorithm for credit card validation",
        "Provide specific error messages and correction suggestions"
      ],
      "solution": "def validate_data_fields(data, validation_rules):\n    import re\n    \n    result = {\n        'valid': True,\n        'field_results': {},\n        'errors': [],\n        'warnings': []\n    }\n    \n    for field_name, rules in validation_rules.items():\n        field_result = {\n            'valid': True,\n            'value': data.get(field_name),\n            'errors': [],\n            'warnings': []\n        }\n        \n        field_value = data.get(field_name)\n        \n        # Check if field is required\n        if rules.get('required', False) and (field_value is None or field_value == ''):\n            field_result['valid'] = False\n            field_result['errors'].append('Field is required')\n            result['valid'] = False\n        \n        # Skip further validation if field is empty and not required\n        if field_value is None or field_value == '':\n            result['field_results'][field_name] = field_result\n            continue\n        \n        # Validate based on data type\n        data_type = rules.get('type', 'string')\n        \n        if data_type == 'email':\n            if not validate_email(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid email format')\n                result['valid'] = False\n        \n        elif data_type == 'phone':\n            phone_result = validate_phone(field_value)\n            if not phone_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(phone_result['error'])\n                result['valid'] = False\n        \n        elif data_type == 'credit_card':\n            cc_result = validate_credit_card(field_value)\n            if not cc_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(cc_result['error'])\n                result['valid'] = False\n            else:\n                field_result['card_type'] = cc_result.get('card_type')\n        \n        elif data_type == 'url':\n            if not validate_url(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid URL format')\n                result['valid'] = False\n        \n        elif data_type == 'pattern':\n            pattern = rules.get('pattern')\n            if pattern and not re.match(pattern, str(field_value)):\n                field_result['valid'] = False\n                field_result['errors'].append(f'Does not match required pattern: {pattern}')\n                result['valid'] = False\n        \n        # Length validation\n        if 'min_length' in rules:\n            if len(str(field_value)) < rules['min_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Minimum length is {rules[\"min_length\"]}')\n                result['valid'] = False\n        \n        if 'max_length' in rules:\n            if len(str(field_value)) > rules['max_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Maximum length is {rules[\"max_length\"]}')\n                result['valid'] = False\n        \n        # Range validation for numbers\n        if data_type in ['int', 'float']:\n            try:\n                num_value = float(field_value)\n                if 'min_value' in rules and num_value < rules['min_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Minimum value is {rules[\"min_value\"]}')\n                    result['valid'] = False\n                \n                if 'max_value' in rules and num_value > rules['max_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Maximum value is {rules[\"max_value\"]}')\n                    result['valid'] = False\n            except ValueError:\n                field_result['valid'] =, cleaned):\n        return {'valid': False, 'error': 'Credit card number must contain only digits'}\n    \n    # Luhn algorithm check\n    def luhn_check(card_num):\n        digits = [int(d) for d in card_num]\n        for i in range(len(digits) - 2, -1, -2):\n            digits[i] *= 2\n            if digits[i] > 9:\n                digits[i] -= 9\n        return sum(digits) % 10 == 0\n    \n    if not luhn_check(cleaned):\n        return {'valid': False, 'error': 'Invalid credit card number (failed Luhn check)'}\n    \n    # Determine card type\n    card_type = 'Unknown'\n    if cleaned.startswith('4'):\n        card_type = 'Visa'\n    elif cleaned.startswith(('51', '52', '53', '54', '55')):\n        card_type = 'MasterCard'\n    elif cleaned.startswith(('34', '37')):\n        card_type = 'American Express'\n    \n    return {'valid': True, 'card_type': card_type}\n\ndef validate_url(url):\n    pattern = r'^https?://(?:[-\\w.])+(?:\\:[0-9]+)?(?:/(?:[\\w/_.])*(?:\\?(?:[\\w&=%.])*)?(?:\\#(?:[\\w.])*)?)?{
  "problems": [
    {
      "id": 129,
      "title": "CSV File Parser and Data Extractor",
      "statement": "Parse a CSV file content and extract specific columns with data validation. Handle various delimiters (comma, semicolon, tab), quoted fields, and missing values. Return structured data with column headers and rows. For example, parse 'Name,Age,City\\nJohn,25,NYC\\nJane,30,LA' and extract specific columns with type conversion.",
      "function_signature": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):",
      "categories": ["input_output_processing", "data_parsing"],
      "python_constructs": ["csv_parsing", "string_manipulation", "data_validation"],
      "hints": [
        "Use csv module or manual parsing to handle different delimiters",
        "Handle quoted fields that may contain delimiter characters",
        "Implement type conversion for numeric columns and date parsing"
      ],
      "solution": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):\n    import csv\n    from io import StringIO\n    \n    if not csv_content:\n        return {'headers': [], 'rows': [], 'error': None}\n    \n    try:\n        # Parse CSV content\n        csv_reader = csv.reader(StringIO(csv_content), delimiter=delimiter)\n        rows = list(csv_reader)\n        \n        if not rows:\n            return {'headers': [], 'rows': [], 'error': None}\n        \n        headers = rows[0]\n        data_rows = rows[1:]\n        \n        # Extract specific columns if requested\n        if extract_columns:\n            try:\n                # Find column indices\n                column_indices = []\n                for col in extract_columns:\n                    if isinstance(col, str):\n                        if col in headers:\n                            column_indices.append(headers.index(col))\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column \"{col}\" not found'}\n                    elif isinstance(col, int):\n                        if 0 <= col < len(headers):\n                            column_indices.append(col)\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column index {col} out of range'}\n                \n                # Extract columns\n                filtered_headers = [headers[i] for i in column_indices]\n                filtered_rows = []\n                \n                for row in data_rows:\n                    filtered_row = []\n                    for i in column_indices:\n                        if i < len(row):\n                            filtered_row.append(row[i])\n                        else:\n                            filtered_row.append('')  # Missing value\n                    filtered_rows.append(filtered_row)\n                \n                headers = filtered_headers\n                data_rows = filtered_rows\n            \n            except Exception as e:\n                return {'headers': [], 'rows': [], 'error': f'Column extraction error: {str(e)}'}\n        \n        # Apply type conversions\n        if convert_types:\n            converted_rows = []\n            for row in data_rows:\n                converted_row = []\n                for i, value in enumerate(row):\n                    if i < len(headers) and headers[i] in convert_types:\n                        try:\n                            converter = convert_types[headers[i]]\n                            if converter == 'int':\n                                converted_row.append(int(value) if value.strip() else None)\n                            elif converter == 'float':\n                                converted_row.append(float(value) if value.strip() else None)\n                            elif converter == 'bool':\n                                converted_row.append(value.lower() in ['true', '1', 'yes', 'on'])\n                            else:\n                                converted_row.append(value)\n                        except (ValueError, AttributeError):\n                            converted_row.append(None)  # Conversion failed\n                    else:\n                        converted_row.append(value)\n                converted_rows.append(converted_row)\n            data_rows = converted_rows\n        \n        return {\n            'headers': headers,\n            'rows': data_rows,\n            'row_count': len(data_rows),\n            'column_count': len(headers),\n            'error': None\n        }\n    \n    except Exception as e:\n        return {'headers': [], 'rows': [], 'error': f'Parsing error: {str(e)}'}"
      ],
      "test_cases": [
        {"input": ["Name,Age,City\\nJohn,25,NYC\\nJane,30,LA", ",", null, null], "expected": {"headers": ["Name", "Age", "City"], "rows": [["John", "25", "NYC"], ["Jane", "30", "LA"]], "row_count": 2, "column_count": 3, "error": null}},
        {"input": ["Name;Age;Salary\\nJohn;25;50000\\nJane;30;60000", ";", ["Name", "Salary"], {"Salary": "int"}], "expected": {"headers": ["Name", "Salary"], "rows": [["John", 50000], ["Jane", 60000]], "row_count": 2, "column_count": 2, "error": null}},
        {"input": ["A,B,C\\n1,2,3", ",", [0, 2], {"A": "int", "C": "int"}], "expected": {"headers": ["A", "C"], "rows": [[1, 3]], "row_count": 1, "column_count": 2, "error": null}},
        {"input": [""], "expected": {"headers": [], "rows": [], "error": null}},
        {"input": ["Name,Age\\nJohn,25", ",", ["NonExistent"], null], "expected": {"headers": [], "rows": [], "error": "Column \"NonExistent\" not found"}}
      ]
    },
    {
      "id": 130,
      "title": "File Content Word and Line Counter",
      "statement": "Count words, lines, characters, and paragraphs in text content. Provide detailed statistics including average word length, sentence count, and most frequent words. Handle different text encodings and empty lines. Return comprehensive text analysis results.",
      "function_signature": "def analyze_text_content(content, include_word_freq=True, top_words=10):",
      "categories": ["input_output_processing", "text_analysis"],
      "python_constructs": ["text_processing", "regular_expressions", "frequency_analysis"],
      "hints": [
        "Use regular expressions to identify words, sentences, and paragraphs",
        "Handle different definitions of word boundaries and punctuation",
        "Calculate frequency distribution and sort by occurrence count"
      ],
      "solution": "def analyze_text_content(content, include_word_freq=True, top_words=10):\n    import re\n    from collections import Counter\n    \n    if not isinstance(content, str):\n        return None\n    \n    if not content:\n        return {\n            'characters': 0,\n            'characters_no_spaces': 0,\n            'words': 0,\n            'lines': 0,\n            'paragraphs': 0,\n            'sentences': 0,\n            'avg_word_length': 0.0,\n            'word_frequency': {} if include_word_freq else None\n        }\n    \n    # Basic counts\n    char_count = len(content)\n    char_count_no_spaces = len(content.replace(' ', '').replace('\\t', '').replace('\\n', '').replace('\\r', ''))\n    \n    # Line count\n    lines = content.split('\\n')\n    line_count = len(lines)\n    \n    # Paragraph count (separated by empty lines)\n    paragraphs = re.split(r'\\n\\s*\\n', content.strip())\n    paragraph_count = len([p for p in paragraphs if p.strip()])\n    \n    # Word extraction and counting\n    words = re.findall(r\"\\b\\w+\\b\", content.lower())\n    word_count = len(words)\n    \n    # Average word length\n    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0.0\n    \n    # Sentence count (approximate)\n    sentences = re.split(r'[.!?]+', content)\n    sentence_count = len([s for s in sentences if s.strip()])\n    \n    result = {\n        'characters': char_count,\n        'characters_no_spaces': char_count_no_spaces,\n        'words': word_count,\n        'lines': line_count,\n        'paragraphs': paragraph_count,\n        'sentences': sentence_count,\n        'avg_word_length': round(avg_word_length, 2)\n    }\n    \n    # Word frequency analysis\n    if include_word_freq and words:\n        word_freq = Counter(words)\n        top_word_list = word_freq.most_common(top_words)\n        result['word_frequency'] = {\n            'total_unique_words': len(word_freq),\n            'top_words': top_word_list,\n            'most_common_word': top_word_list[0] if top_word_list else None\n        }\n    else:\n        result['word_frequency'] = None\n    \n    return result",
      "test_cases": [
        {"input": ["Hello world! This is a test.\\nAnother line here.", true, 5], "expected": {"characters": 46, "characters_no_spaces": 37, "words": 9, "lines": 2, "paragraphs": 1, "sentences": 2, "avg_word_length": 4.0, "word_frequency": {"total_unique_words": 9, "top_words": [["hello", 1], ["world", 1], ["this", 1], ["is", 1], ["a", 1]], "most_common_word": ["hello", 1]}}},
        {"input": [""], "expected": {"characters": 0, "characters_no_spaces": 0, "words": 0, "lines": 0, "paragraphs": 0, "sentences": 0, "avg_word_length": 0.0, "word_frequency": {}}},
        {"input": ["Word word WORD!", false, 3], "expected": {"characters": 14, "characters_no_spaces": 11, "words": 3, "lines": 1, "paragraphs": 1, "sentences": 1, "avg_word_length": 4.0, "word_frequency": null}},
        {"input": ["Line 1\\n\\nParagraph 2\\nStill paragraph 2", true, 3], "expected": {"characters": 35, "characters_no_spaces": 25, "words": 6, "lines": 4, "paragraphs": 2, "sentences": 1, "avg_word_length": 4.17, "word_frequency": {"total_unique_words": 6, "top_words": [["line", 1], ["1", 1], ["paragraph", 2]], "most_common_word": ["paragraph", 2]}}},
        {"input": ["Hello! How are you? I am fine.", true, 10], "expected": {"characters": 30, "characters_no_spaces": 24, "words": 7, "lines": 1, "paragraphs": 1, "sentences": 3, "avg_word_length": 3.0, "word_frequency": {"total_unique_words": 7, "top_words": [["hello", 1], ["how", 1], ["are", 1], ["you", 1], ["i", 1], ["am", 1], ["fine", 1]], "most_common_word": ["hello", 1]}}}
      ]
    },
    {
      "id": 131,
      "title": "JSON Data Parser and Validator",
      "statement": "Parse and validate JSON data with schema checking, nested object extraction, and data transformation. Handle malformed JSON, validate data types, and extract specific fields using JSONPath-like syntax. Return parsed data with validation results and extracted fields.",
      "function_signature": "def parse_json_data(json_string, schema=None, extract_paths=None):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["json_parsing", "data_validation", "nested_data_access"],
      "hints": [
        "Use json module for parsing and handle JSONDecodeError exceptions",
        "Implement schema validation for required fields and data types",
        "Support dot notation for nested field extraction"
      ],
      "solution": "def parse_json_data(json_string, schema=None, extract_paths=None):\n    import json\n    \n    result = {\n        'valid': False,\n        'data': None,\n        'errors': [],\n        'extracted_fields': {},\n        'schema_valid': True\n    }\n    \n    # Parse JSON\n    try:\n        data = json.loads(json_string)\n        result['valid'] = True\n        result['data'] = data\n    except json.JSONDecodeError as e:\n        result['errors'].append(f'JSON parsing error: {str(e)}')\n        return result\n    except Exception as e:\n        result['errors'].append(f'Unexpected error: {str(e)}')\n        return result\n    \n    # Schema validation\n    if schema:\n        schema_errors = validate_schema(data, schema)\n        if schema_errors:\n            result['schema_valid'] = False\n            result['errors'].extend(schema_errors)\n    \n    # Extract specific fields\n    if extract_paths:\n        for path_name, path in extract_paths.items():\n            try:\n                extracted_value = extract_nested_value(data, path)\n                result['extracted_fields'][path_name] = extracted_value\n            except Exception as e:\n                result['errors'].append(f'Extraction error for \"{path_name}\": {str(e)}')\n                result['extracted_fields'][path_name] = None\n    \n    return result\n\ndef validate_schema(data, schema):\n    errors = []\n    \n    # Check required fields\n    if 'required' in schema:\n        for field in schema['required']:\n            if field not in data:\n                errors.append(f'Missing required field: {field}')\n    \n    # Check field types\n    if 'properties' in schema:\n        for field, field_schema in schema['properties'].items():\n            if field in data:\n                expected_type = field_schema.get('type')\n                actual_value = data[field]\n                \n                if expected_type == 'string' and not isinstance(actual_value, str):\n                    errors.append(f'Field \"{field}\" should be string, got {type(actual_value).__name__}')\n                elif expected_type == 'number' and not isinstance(actual_value, (int, float)):\n                    errors.append(f'Field \"{field}\" should be number, got {type(actual_value).__name__}')\n                elif expected_type == 'integer' and not isinstance(actual_value, int):\n                    errors.append(f'Field \"{field}\" should be integer, got {type(actual_value).__name__}')\n                elif expected_type == 'boolean' and not isinstance(actual_value, bool):\n                    errors.append(f'Field \"{field}\" should be boolean, got {type(actual_value).__name__}')\n                elif expected_type == 'array' and not isinstance(actual_value, list):\n                    errors.append(f'Field \"{field}\" should be array, got {type(actual_value).__name__}')\n                elif expected_type == 'object' and not isinstance(actual_value, dict):\n                    errors.append(f'Field \"{field}\" should be object, got {type(actual_value).__name__}')\n    \n    return errors\n\ndef extract_nested_value(data, path):\n    \"\"\"Extract value using dot notation path like 'user.profile.name'\"\"\"\n    keys = path.split('.')\n    current = data\n    \n    for key in keys:\n        if isinstance(current, dict) and key in current:\n            current = current[key]\n        elif isinstance(current, list) and key.isdigit():\n            index = int(key)\n            if 0 <= index < len(current):\n                current = current[index]\n            else:\n                raise KeyError(f'List index {index} out of range')\n        else:\n            raise KeyError(f'Key \"{key}\" not found in path \"{path}\"')\n    \n    return current",
      "test_cases": [
        {"input": ["{\"name\": \"John\", \"age\": 30, \"city\": \"NYC\"}", null, null], "expected": {"valid": true, "data": {"name": "John", "age": 30, "city": "NYC"}, "errors": [], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": \"John\", \"age\": 30}", {"required": ["name", "age"], "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}, {"user_name": "name", "user_age": "age"}], "expected": {"valid": true, "data": {"name": "John", "age": 30}, "errors": [], "extracted_fields": {"user_name": "John", "user_age": 30}, "schema_valid": true}},
        {"input": ["{\"user\": {\"profile\": {\"name\": \"Alice\"}}}", null, {"nested_name": "user.profile.name"}], "expected": {"valid": true, "data": {"user": {"profile": {"name": "Alice"}}}, "errors": [], "extracted_fields": {"nested_name": "Alice"}, "schema_valid": true}},
        {"input": ["invalid json", null, null], "expected": {"valid": false, "data": null, "errors": ["JSON parsing error: Expecting value: line 1 column 1 (char 0)"], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": 123}", {"required": ["name"], "properties": {"name": {"type": "string"}}}, null], "expected": {"valid": true, "data": {"name": 123}, "errors": ["Field \"name\" should be string, got int"], "extracted_fields": {}, "schema_valid": false}}
      ]
    },
    {
      "id": 132,
      "title": "Command Line Arguments Processor",
      "statement": "Process command line arguments with support for flags, options with values, positional arguments, and help generation. Handle short (-h) and long (--help) options, validate required arguments, and provide default values. Return parsed arguments dictionary with validation results.",
      "function_signature": "def process_command_args(args_list, arg_definitions):",
      "categories": ["input_output_processing", "argument_parsing"],
      "python_constructs": ["argument_parsing", "command_line_interface", "validation_logic"],
      "hints": [
        "Parse arguments into flags (boolean), options (key-value), and positional arguments",
        "Support both short (-h) and long (--help) argument formats",
        "Validate required arguments and apply default values"
      ],
      "solution": "def process_command_args(args_list, arg_definitions):\n    result = {\n        'parsed_args': {},\n        'positional_args': [],\n        'errors': [],\n        'help_requested': False\n    }\n    \n    if not args_list:\n        args_list = []\n    \n    # Check for help request\n    if '-h' in args_list or '--help' in args_list:\n        result['help_requested'] = True\n        result['help_text'] = generate_help_text(arg_definitions)\n        return result\n    \n    # Initialize with default values\n    for arg_name, arg_config in arg_definitions.items():\n        if 'default' in arg_config:\n            result['parsed_args'][arg_name] = arg_config['default']\n    \n    i = 0\n    while i < len(args_list):\n        arg = args_list[i]\n        \n        if arg.startswith('--'):\n            # Long option\n            option_name = arg[2:]\n            arg_config = find_arg_config(option_name, arg_definitions, 'long')\n            \n            if not arg_config:\n                result['errors'].append(f'Unknown option: {arg}')\n                i += 1\n                continue\n            \n            arg_name, config = arg_config\n            \n            if config.get('type') == 'flag':\n                result['parsed_args'][arg_name] = True\n                i += 1\n            else:\n                # Option with value\n                if i + 1 >= len(args_list):\n                    result['errors'].append(f'Option {arg} requires a value')\n                    i += 1\n                    continue\n                \n                value = args_list[i + 1]\n                converted_value = convert_arg_value(value, config.get('value_type', 'string'))\n                \n                if converted_value is None and config.get('value_type') != 'string':\n                    result['errors'].append(f'Invalid {config.get(\"value_type\")} value for {arg}: {value}')\n                else:\n                    result['parsed_args'][arg_name] = converted_value if converted_value is not None else value\n                \n                i += 2\n        \n        elif arg.startswith('-') and len(arg) > 1:\n            # Short option(s)\n            for char in arg[1:]:\n                arg_config = find_arg_config(char, arg_definitions, 'short')\n                \n                if not arg_config:\n                    result['errors'].append(f'Unknown option: -{char}')\n                    continue\n                \n                arg_name, config = arg_config\n                \n                if config.get('type') == 'flag':\n                    result['parsed_args'][arg_name] = True\n            i += 1\n        \n        else:\n            # Positional argument\n            result['positional_args'].append(arg)\n            i += 1\n    \n    # Validate required arguments\n    for arg_name, arg_config in arg_definitions.items():\n        if arg_config.get('required', False) and arg_name not in result['parsed_args']:\n            result['errors'].append(f'Required argument missing: {arg_name}')\n    \n    return result\n\ndef find_arg_config(option, arg_definitions, option_type):\n    for arg_name, config in arg_definitions.items():\n        if option_type == 'long' and config.get('long') == option:\n            return (arg_name, config)\n        elif option_type == 'short' and config.get('short') == option:\n            return (arg_name, config)\n    return None\n\ndef convert_arg_value(value, value_type):\n    try:\n        if value_type == 'int':\n            return int(value)\n        elif value_type == 'float':\n            return float(value)\n        elif value_type == 'bool':\n            return value.lower() in ['true', '1', 'yes', 'on']\n        else:\n            return value\n    except (ValueError, AttributeError):\n        return None\n\ndef generate_help_text(arg_definitions):\n    help_lines = ['Available options:']\n    \n    for arg_name, config in arg_definitions.items():\n        line_parts = []\n        \n        if config.get('short'):\n            line_parts.append(f'-{config[\"short\"]}')\n        \n        if config.get('long'):\n            line_parts.append(f'--{config[\"long\"]}')\n        \n        option_text = ', '.join(line_parts)\n        \n        if config.get('description'):\n            option_text += f': {config[\"description\"]}'\n        \n        if config.get('required'):\n            option_text += ' (required)'\n        \n        if 'default' in config:\n            option_text += f' (default: {config[\"default\"]})')\n        \n        help_lines.append(f'  {option_text}')\n    \n    return '\\n'.join(help_lines)",
      "test_cases": [
        {"input": [["--input", "file.txt", "-v", "--output", "result.txt"], {"input_file": {"long": "input", "required": true}, "verbose": {"short": "v", "type": "flag"}, "output_file": {"long": "output", "default": "output.txt"}}], "expected": {"parsed_args": {"input_file": "file.txt", "verbose": true, "output_file": "result.txt"}, "positional_args": [], "errors": [], "help_requested": false}},
        {"input": [["-h"], {"verbose": {"short": "v", "type": "flag"}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": [], "help_requested": true, "help_text": "Available options:\\n  -v: (required)"}},
        {"input": [["--count", "5", "file1", "file2"], {"count": {"long": "count", "value_type": "int", "default": 1}}], "expected": {"parsed_args": {"count": 5}, "positional_args": ["file1", "file2"], "errors": [], "help_requested": false}},
        {"input": [["--missing"], {"required_arg": {"long": "required", "required": true}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": ["Unknown option: --missing", "Required argument missing: required_arg"], "help_requested": false}},
        {"input": [[], {"default_val": {"long": "default", "default": "test"}}], "expected": {"parsed_args": {"default_val": "test"}, "positional_args": [], "errors": [], "help_requested": false}}
      ]
    },
    {
      "id": 133,
      "title": "Data Validation Engine",
      "statement": "Validate various data types including email addresses, phone numbers, credit card numbers, URLs, and custom patterns. Return detailed validation results with specific error messages and suggestions for correction. Support multiple validation rules per field.",
      "function_signature": "def validate_data_fields(data, validation_rules):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["regular_expressions", "validation_patterns", "error_handling"],
      "hints": [
        "Use regular expressions for pattern matching (email, phone, URL)",
        "Implement Luhn algorithm for credit card validation",
        "Provide specific error messages and correction suggestions"
      ],
      "solution": "def validate_data_fields(data, validation_rules):\n    import re\n    \n    result = {\n        'valid': True,\n        'field_results': {},\n        'errors': [],\n        'warnings': []\n    }\n    \n    for field_name, rules in validation_rules.items():\n        field_result = {\n            'valid': True,\n            'value': data.get(field_name),\n            'errors': [],\n            'warnings': []\n        }\n        \n        field_value = data.get(field_name)\n        \n        # Check if field is required\n        if rules.get('required', False) and (field_value is None or field_value == ''):\n            field_result['valid'] = False\n            field_result['errors'].append('Field is required')\n            result['valid'] = False\n        \n        # Skip further validation if field is empty and not required\n        if field_value is None or field_value == '':\n            result['field_results'][field_name] = field_result\n            continue\n        \n        # Validate based on data type\n        data_type = rules.get('type', 'string')\n        \n        if data_type == 'email':\n            if not validate_email(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid email format')\n                result['valid'] = False\n        \n        elif data_type == 'phone':\n            phone_result = validate_phone(field_value)\n            if not phone_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(phone_result['error'])\n                result['valid'] = False\n        \n        elif data_type == 'credit_card':\n            cc_result = validate_credit_card(field_value)\n            if not cc_result['valid']:\n                field_result['valid'] = False\n                field_result['errors'].append(cc_result['error'])\n                result['valid'] = False\n            else:\n                field_result['card_type'] = cc_result.get('card_type')\n        \n        elif data_type == 'url':\n            if not validate_url(field_value):\n                field_result['valid'] = False\n                field_result['errors'].append('Invalid URL format')\n                result['valid'] = False\n        \n        elif data_type == 'pattern':\n            pattern = rules.get('pattern')\n            if pattern and not re.match(pattern, str(field_value)):\n                field_result['valid'] = False\n                field_result['errors'].append(f'Does not match required pattern: {pattern}')\n                result['valid'] = False\n        \n        # Length validation\n        if 'min_length' in rules:\n            if len(str(field_value)) < rules['min_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Minimum length is {rules[\"min_length\"]}')\n                result['valid'] = False\n        \n        if 'max_length' in rules:\n            if len(str(field_value)) > rules['max_length']:\n                field_result['valid'] = False\n                field_result['errors'].append(f'Maximum length is {rules[\"max_length\"]}')\n                result['valid'] = False\n        \n        # Range validation for numbers\n        if data_type in ['int', 'float']:\n            try:\n                num_value = float(field_value)\n                if 'min_value' in rules and num_value < rules['min_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Minimum value is {rules[\"min_value\"]}')\n                    result['valid'] = False\n                \n                if 'max_value' in rules and num_value > rules['max_value']:\n                    field_result['valid'] = False\n                    field_result['errors'].append(f'Maximum value is {rules[\"max_value\"]}')\n                    result['valid'] = False\n            except ValueError:\n                field_result['valid'] =\n    return re.match(pattern, url) is not None",
      "test_cases": [
        {"input": [{"email": "test@example.com", "phone": "123-456-7890"}, {"email": {"type": "email", "required": true}, "phone": {"type": "phone", "required": true}}], "expected": {"valid": true, "field_results": {"email": {"valid": true, "value": "test@example.com", "errors": [], "warnings": []}, "phone": {"valid": true, "value": "123-456-7890", "errors": [], "warnings": []}}, "errors": [], "warnings": []}},
        {"input": [{"email": "invalid-email", "age": "25"}, {"email": {"type": "email", "required": true}, "age": {"type": "int", "min_value": 0, "max_value": 120}}], "expected": {"valid": false, "field_results": {"email": {"valid": false, "value": "invalid-email", "errors": ["Invalid email format"], "warnings": []}, "age": {"valid": true, "value": "25", "errors": [], "warnings": []}}, "errors": [], "warnings": []}},
        {"input": [{"card": "4532015112830366"}, {"card": {"type": "credit_card"}}], "expected": {"valid": true, "field_results": {"card": {"valid": true, "value": "4532015112830366", "errors": [], "warnings": [], "card_type": "Visa"}}, "errors": [], "warnings": []}},
        {"input": [{"url": "https://example.com/path?query=1"}, {"url": {"type": "url", "required": true}}], "expected": {"valid": true, "field_results": {"url": {"valid": true, "value": "https://example.com/path?query=1", "errors": [], "warnings": []}}, "errors": [], "warnings": []}},
        {"input": [{"name": ""}, {"name": {"type": "string", "required": true, "min_length": 2}}], "expected": {"valid": false, "field_results": {"name": {"valid": false, "value": "", "errors": ["Field is required"], "warnings": []}}, "errors": [], "warnings": []}}
      ]
    },
    {
      "id": 134,
      "title": "Data Format Converter",
      "statement": "Convert data between different formats: CSV  JSON, XML  JSON, YAML  JSON. Preserve data structure and handle nested objects appropriately. Support custom delimiters for CSV and maintain data types during conversion. Return converted data with conversion statistics.",
      "function_signature": "def convert_data_format(data, from_format, to_format, options=None):",
      "categories": ["input_output_processing", "format_conversion"],
      "python_constructs": ["format_conversion", "data_serialization", "multiple_formats"],
      "hints": [
        "Use appropriate libraries: json, csv, xml.etree.ElementTree",
        "Handle data type preservation during conversions",
        "Support nested structures and arrays in conversions"
      ],
      "solution": "def convert_data_format(data, from_format, to_format, options=None):\n    import json\n    import csv\n    from io import StringIO\n    \n    if options is None:\n        options = {}\n    \n    result = {\n        'success': False,\n        'converted_data': None,\n        'statistics': {},\n        'errors': []\n    }\n    \n    try:\n        # Parse input data based on format\n        if from_format == 'csv':\n            parsed_data = parse_csv_to_dict(data, options.get('delimiter', ','))\n        elif from_format == 'json':\n            if isinstance(data, str):\n                parsed_data = json.loads(data)\n            else:\n                parsed_data = data\n        elif from_format == 'xml':\n            parsed_data = parse_xml_to_dict(data)\n        else:\n            result['errors'].append(f'Unsupported input format: {from_format}')\n            return result\n        \n        # Convert to target format\n        if to_format == 'json':\n            converted = json.dumps(parsed_data, indent=2, ensure_ascii=False)\n        elif to_format == 'csv':\n            converted = convert_to_csv(parsed_data, options.get('delimiter', ','))\n        elif to_format == 'xml':\n            converted = convert_to_xml(parsed_data, options.get('root_element', 'root'))\n        else:\n            result['errors'].append(f'Unsupported output format: {to_format}')\n            return result\n        \n        result['success'] = True\n        result['converted_data'] = converted\n        result['statistics'] = calculate_conversion_stats(parsed_data, from_format, to_format)\n        \n    except Exception as e:\n        result['errors'].append(f'Conversion error: {str(e)}')\n    \n    return result\n\ndef parse_csv_to_dict(csv_data, delimiter):\n    \"\"\"Convert CSV string to list of dictionaries\"\"\"\n    csv_reader = csv.DictReader(StringIO(csv_data), delimiter=delimiter)\n    return list(csv_reader)\n\ndef convert_to_csv(data, delimiter):\n    \"\"\"Convert list of dictionaries to CSV string\"\"\"\n    if not data:\n        return ''\n    \n    if not isinstance(data, list):\n        data = [data]\n    \n    # Get all unique keys\n    all_keys = set()\n    for item in data:\n        if isinstance(item, dict):\n            all_keys.update(item.keys())\n    \n    output = StringIO()\n    writer = csv.DictWriter(output, fieldnames=sorted(all_keys), delimiter=delimiter)\n    writer.writeheader()\n    \n    for item in data:\n        if isinstance(item, dict):\n            writer.writerow(item)\n    \n    return output.getvalue()\n\ndef parse_xml_to_dict(xml_data):\n    \"\"\"Convert XML string to dictionary (simplified)\"\"\"\n    import xml.etree.ElementTree as ET\n    \n    root = ET.fromstring(xml_data)\n    \n    def xml_to_dict(element):\n        result = {}\n        \n        # Add attributes\n        if element.attrib:\n            result['@attributes'] = element.attrib\n        \n        # Add text content\n        if element.text and element.text.strip():\n            if len(element) == 0:  # No children\n                return element.text.strip()\n            else:\n                result['#text'] = element.text.strip()\n        \n        # Add children\n        for child in element:\n            child_data = xml_to_dict(child)\n            if child.tag in result:\n                if not isinstance(result[child.tag], list):\n                    result[child.tag] = [result[child.tag]]\n                result[child.tag].append(child_data)\n            else:\n                result[child.tag] = child_data\n        \n        return result\n    \n    return {root.tag: xml_to_dict(root)}\n\ndef convert_to_xml(data, root_element):\n    \"\"\"Convert dictionary to XML string (simplified)\"\"\"\n    import xml.etree.ElementTree as ET\n    \n    def dict_to_xml(element, data):\n        if isinstance(data, dict):\n            for key, value in data.items():\n                if key == '@attributes':\n                    element.attrib.update(value)\n                elif key == '#text':\n                    element.text = str(value)\n                else:\n                    child = ET.SubElement(element, key)\n                    dict_to_xml(child, value)\n        elif isinstance(data, list):\n            for item in data:\n                child = ET.SubElement(element, 'item')\n                dict_to_xml(child, item)\n        else:\n            element.text = str(data)\n    \n    root = ET.Element(root_element)\n    dict_to_xml(root, data)\n    \n    return ET.tostring(root, encoding='unicode')\n\ndef calculate_conversion_stats(data, from_format, to_format):\n    \"\"\"Calculate statistics about the conversion\"\"\"\n    stats = {\n        'from_format': from_format,\n        'to_format': to_format,\n        'data_type': type(data).__name__\n    }\n    \n    if isinstance(data, list):\n        stats['record_count'] = len(data)\n        if data and isinstance(data[0], dict):\n            stats['field_count'] = len(data[0].keys())\n    elif isinstance(data, dict):\n        stats['field_count'] = len(data.keys())\n    \n    return stats",
      "test_cases": [
        {"input": ["name,age\\nJohn,25\\nJane,30", "csv", "json", null], "expected": {"success": true, "converted_data": "[\\n  {\\n    \"name\": \"John\",\\n    \"age\": \"25\"\\n  },\\n  {\\n    \"name\": \"Jane\",\\n    \"age\": \"30\"\\n  }\\n]", "statistics": {"from_format": "csv", "to_format": "json", "data_type": "list", "record_count": 2, "field_count": 2}, "errors": []}},
        {"input": [[{"name": "John", "age": 25}], "json", "csv", {"delimiter": ","}], "expected": {"success": true, "converted_data": "age,name\\r\\n25,John\\r\\n", "statistics": {"from_format": "json", "to_format": "csv", "data_type": "list", "record_count": 1, "field_count": 2}, "errors": []}},
        {"input": ["<root><person><name>John</name><age>25</age></person></root>", "xml", "json", null], "expected": {"success": true, "converted_data": "{\\n  \"root\": {\\n    \"person\": {\\n      \"name\": \"John\",\\n      \"age\": \"25\"\\n    }\\n  }\\n}", "statistics": {"from_format": "xml", "to_format": "json", "data_type": "dict", "field_count": 1}, "errors": []}},
        {"input": [{"name": "John", "age": 25}, "json", "xml", {"root_element": "person"}], "expected": {"success": true, "converted_data": "<person><name>John</name><age>25</age></person>", "statistics": {"from_format": "json", "to_format": "xml", "data_type": "dict", "field_count": 2}, "errors": []}},
        {"input": ["invalid,csv,data\\nno,matching", "csv", "invalid_format", null], "expected": {"success": false, "converted_data": null, "statistics": {}, "errors": ["Unsupported output format: invalid_format"]}}
      ]
    },
    {
      "id": 135,
      "title": "Log File Pattern Analyzer",
      "statement": "Analyze log files to extract patterns, error counts, IP addresses, timestamps, and response codes. Support different log formats (Apache, Nginx, custom) and generate summary statistics. Return analysis results with pattern matching and anomaly detection.",
      "function_signature": "def analyze_log_patterns(log_content, log_format='apache', filters=None):",
      "categories": ["input_output_processing", "pattern_analysis"],
      "python_constructs": ["regex_patterns", "log_parsing", "statistical_analysis"],
      "hints": [
        "Define regex patterns for different log formats (Apache Common Log, Combined Log)",
        "Extract key fields: IP, timestamp, method, URL, status code, size",
        "Calculate statistics and identify patterns or anomalies"
      ],
      "solution": "def analyze_log_patterns(log_content, log_format='apache', filters=None):\n    import re\n    from collections import Counter, defaultdict\n    from datetime import datetime\n    \n    result = {\n        'total_lines': 0,\n        'parsed_lines': 0,\n        'errors': [],\n        'statistics': {},\n        'patterns': {},\n        'anomalies': []\n    }\n    \n    if not log_content:\n        return result\n    \n    # Define log format patterns\n    patterns = {\n        'apache': r'(\\S+) \\S+ \\S+ \\[([^\\]]+)\\] \"(\\S+) (\\S+) \\S+\" (\\d+) (\\S+)',\n        'nginx': r'(\\S+) - \\S+ \\[([^\\]]+)\\] \"(\\S+) (\\S+) \\S+\" (\\d+) (\\d+) \"([^\"]*)\" \"([^\"]*)\"',\n        'custom': r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (\\w+) (.*)',\n        'iis': r'\\S+ \\S+ (\\S+) (\\S+) \\S+ (\\S+) (\\d+) (\\d+) (\\d+) (\\d+)'\n    }\n    \n    if log_format not in patterns:\n        result['errors'].append(f'Unsupported log format: {log_format}')\n        return result\n    \n    pattern = patterns[log_format]\n    lines = log_content.strip().split('\\n')\n    result['total_lines'] = len(lines)\n    \n    # Data collection\n    parsed_entries = []\n    ip_addresses = Counter()\n    status_codes = Counter()\n    request_methods = Counter()\n    urls = Counter()\n    error_lines = []\n    hourly_distribution = defaultdict(int)\n    \n    for line_num, line in enumerate(lines, 1):\n        line = line.strip()\n        if not line:\n            continue\n        \n        match = re.match(pattern, line)\n        if match:\n            result['parsed_lines'] += 1\n            \n            if log_format == 'apache':\n                ip, timestamp, method, url, status, size = match.groups()\n                entry = {\n                    'ip': ip,\n                    'timestamp': timestamp,\n                    'method': method,\n                    'url': url,\n                    'status': int(status),\n                    'size': size if size != '-' else 0\n                }\n            \n            elif log_format == 'nginx':\n                ip, timestamp, method, url, status, size, referer, user_agent = match.groups()\n                entry = {\n                    'ip': ip,\n                    'timestamp': timestamp,\n                    'method': method,\n                    'url': url,\n                    'status': int(status),\n                    'size': int(size),\n                    'referer': referer,\n                    'user_agent': user_agent\n                }\n            \n            elif log_format == 'custom':\n                timestamp, level, message = match.groups()\n                entry = {\n                    'timestamp': timestamp,\n                    'level': level,\n                    'message': message\n                }\n            \n            # Apply filters if provided\n            if filters:\n                skip_entry = False\n                for filter_key, filter_value in filters.items():\n                    if filter_key in entry and entry[filter_key] != filter_value:\n                        skip_entry = True\n                        break\n                if skip_entry:\n                    continue\n            \n            parsed_entries.append(entry)\n            \n            # Collect statistics\n            if 'ip' in entry:\n                ip_addresses[entry['ip']] += 1\n            if 'status' in entry:\n                status_codes[entry['status']] += 1\n            if 'method' in entry:\n                request_methods[entry['method']] += 1\n            if 'url' in entry:\n                urls[entry['url']] += 1\n            \n            # Extract hour from timestamp for distribution analysis\n            if 'timestamp' in entry:\n                try:\n                    if log_format in ['apache', 'nginx']:\n                        # Parse Apache/Nginx timestamp format\n                        dt = datetime.strptime(entry['timestamp'].split()[0], '%d/%b/%Y:%H:%M:%S')\n                    else:\n                        # Parse custom timestamp format\n                        dt = datetime.strptime(entry['timestamp'], '%Y-%m-%d %H:%M:%S')\n                    hourly_distribution[dt.hour] += 1\n                except ValueError:\n                    pass\n        \n        else:\n            error_lines.append({'line_number': line_num, 'content': line})\n    \n    # Generate statistics\n    result['statistics'] = {\n        'top_ips': ip_addresses.most_common(10),\n        'status_code_distribution': dict(status_codes),\n        'request_methods': dict(request_methods),\n        'top_urls': urls.most_common(10),\n        'hourly_distribution': dict(hourly_distribution),\n        'error_rate': len([e for e in parsed_entries if e.get('status', 0) >= 400]) / len(parsed_entries) if parsed_entries else 0\n    }\n    \n    # Detect patterns and anomalies\n    result['patterns'] = detect_log_patterns(parsed_entries)\n    result['anomalies'] = detect_anomalies(parsed_entries, ip_addresses, status_codes)\n    \n    if error_lines:\n        result['errors'].extend([f'Failed to parse line {e[\"line_number\"]}: {e[\"content\"][:50]}...' for e in error_lines[:5]])\n    \n    return result\n\ndef detect_log_patterns(entries):\n    \"\"\"Detect common patterns in log entries\"\"\"\n    patterns = {\n        'suspicious_ips': [],\n        'frequent_errors': [],\n        'unusual_user_agents': []\n    }\n    \n    # Find IPs with high error rates\n    ip_stats = defaultdict(lambda: {'total': 0, 'errors': 0})\n    \n    for entry in entries:\n        ip = entry.get('ip')\n        status = entry.get('status', 200)\n        \n        if ip:\n            ip_stats[ip]['total'] += 1\n            if status >= 400:\n                ip_stats[ip]['errors'] += 1\n    \n    for ip, stats in ip_stats.items():\n        if stats['total'] > 10 and stats['errors'] / stats['total'] > 0.5:\n            patterns['suspicious_ips'].append({\n                'ip': ip,\n                'error_rate': stats['errors'] / stats['total'],\n                'total_requests': stats['total']\n            })\n    \n    return patterns\n\ndef detect_anomalies(entries, ip_addresses, status_codes):\n    \"\"\"Detect anomalies in log data\"\"\"\n    anomalies = []\n    \n    # Detect IP addresses with unusually high request volume\n    if ip_addresses:\n        avg_requests = sum(ip_addresses.values()) / len(ip_addresses)\n        threshold = avg_requests * 5  # 5x average\n        \n        for ip, count in ip_addresses.items():\n            if count > threshold:\n                anomalies.append({\n                    'type': 'high_volume_ip',\n                    'ip': ip,\n                    'request_count': count,\n                    'threshold': threshold\n                })\n    \n    # Detect unusual status code patterns\n    total_requests = sum(status_codes.values())\n    if total_requests > 0:\n        error_rate = sum(count for status, count in status_codes.items() if status >= 400) / total_requests\n        if error_rate > 0.1:  # More than 10% errors\n            anomalies.append({\n                'type': 'high_error_rate',\n                'error_rate': error_rate,\n                'total_requests': total_requests\n            })\n    \n    return anomalies",
      "test_cases": [
        {"input": ["192.168.1.1 - - [10/Oct/2023:13:55:36 +0000] \"GET /index.html HTTP/1.1\" 200 1234\\n192.168.1.2 - - [10/Oct/2023:13:55:37 +0000] \"POST /api/login HTTP/1.1\" 404 567", "apache", null], "expected": {"total_lines": 2, "parsed_lines": 2, "errors": [], "statistics": {"top_ips": [["192.168.1.1", 1], ["192.168.1.2", 1]], "status_code_distribution": {200: 1, 404: 1}, "request_methods": {"GET": 1, "POST": 1}, "top_urls": [["/index.html", 1], ["/api/login", 1]], "hourly_distribution": {13: 2}, "error_rate": 0.5}, "patterns": {"suspicious_ips": [], "frequent_errors": [], "unusual_user_agents": []}, "anomalies": [{"type": "high_error_rate", "error_rate": 0.5, "total_requests": 2}]}},
        {"input": ["2023-10-10 14:30:15 ERROR Database connection failed\\n2023-10-10 14:30:16 INFO User logged in", "custom", {"level": "ERROR"}], "expected": {"total_lines": 2, "parsed_lines": 2, "errors": [], "statistics": {"top_ips": [], "status_code_distribution": {}, "request_methods": {}, "top_urls": [], "hourly_distribution": {14: 1}, "error_rate": 0}, "patterns": {"suspicious_ips": [], "frequent_errors": [], "unusual_user_agents": []}, "anomalies": []}},
        {"input": ["invalid log line format", "apache", null], "expected": {"total_lines": 1, "parsed_lines": 0, "errors": ["Failed to parse line 1: invalid log line format"], "statistics": {"top_ips": [], "status_code_distribution": {}, "request_methods": {}, "top_urls": [], "hourly_distribution": {}, "error_rate": 0}, "patterns": {"suspicious_ips": [], "frequent_errors": [], "unusual_user_agents": []}, "anomalies": []}},
        {"input": ["", "apache", null], "expected": {"total_lines": 0, "parsed_lines": 0, "errors": [], "statistics": {}, "patterns": {}, "anomalies": []}},
        {"input": ["192.168.1.1 - - [10/Oct/2023:13:55:36 +0000] \"GET /index.html HTTP/1.1\" 200 1234", "unsupported", null], "expected": {"total_lines": 0, "parsed_lines": 0, "errors": ["Unsupported log format: unsupported"], "statistics": {}, "patterns": {}, "anomalies": []}}
      ]
    },
    {
      "id": 136,
      "title": "Statistical Data Calculator",
      "statement": "Calculate comprehensive statistics from numerical data including mean, median, mode, standard deviation, quartiles, and percentiles. Handle missing values, outliers, and provide data distribution analysis. Support both population and sample statistics.",
      "function_signature": "def calculate_data_statistics(data, remove_outliers=False, sample_stats=True):",
      "categories": ["input_output_processing", "statistical_analysis"],
      "python_constructs": ["statistical_calculations", "data_analysis", "mathematical_operations"],
      "hints": [
        "Implement basic statistical measures: mean, median, mode, standard deviation",
        "Calculate quartiles and percentiles for distribution analysis",
        "Handle outliers using IQR method and provide both filtered and unfiltered results"
      ],
      "solution": "def calculate_data_statistics(data, remove_outliers=False, sample_stats=True):\n    import math\n    from collections import Counter\n    \n    result = {\n        'valid': False,\n        'statistics': {},\n        'distribution': {},\n        'outliers': [],\n        'errors': []\n    }\n    \n    # Validate and clean data\n    if not data:\n        result['errors'].append('No data provided')\n        return result\n    \n    # Convert to numeric and filter out invalid values\n    numeric_data = []\n    invalid_count = 0\n    \n    for value in data:\n        try:\n            if value is not None and str(value).strip() != '':\n                numeric_data.append(float(value))\n            else:\n                invalid_count += 1\n        except (ValueError, TypeError):\n            invalid_count += 1\n    \n    if not numeric_data:\n        result['errors'].append('No valid numeric data found')\n        return result\n    \n    original_data = numeric_data[:]\n    n = len(numeric_data)\n    \n    # Detect and optionally remove outliers\n    outliers = detect_outliers(numeric_data)\n    result['outliers'] = outliers\n    \n    if remove_outliers and outliers:\n        outlier_values = [o['value'] for o in outliers]\n        numeric_data = [x for x in numeric_data if x not in outlier_values]\n        if not numeric_data:\n            result['errors'].append('All data points are outliers')\n            return result\n    \n    # Calculate basic statistics\n    n_filtered = len(numeric_data)\n    sorted_data = sorted(numeric_data)\n    \n    # Mean\n    mean = sum(numeric_data) / n_filtered\n    \n    # Median\n    median = calculate_median(sorted_data)\n    \n    # Mode\n    mode_info = calculate_mode(numeric_data)\n    \n    # Standard deviation and variance\n    variance = sum((x - mean) ** 2 for x in numeric_data) / (n_filtered - 1 if sample_stats and n_filtered > 1 else n_filtered)\n    std_dev = math.sqrt(variance)\n    \n    # Quartiles and percentiles\n    q1 = calculate_percentile(sorted_data, 25)\n    q3 = calculate_percentile(sorted_data, 75)\n    iqr = q3 - q1\n    \n    # Range\n    data_range = max(numeric_data) - min(numeric_data)\n    \n    # Percentiles\n    percentiles = {}\n    for p in [5, 10, 25, 50, 75, 90, 95]:\n        percentiles[f'p{p}'] = calculate_percentile(sorted_data, p)\n    \n    result['statistics'] = {\n        'count': n_filtered,\n        'original_count': n,\n        'invalid_count': invalid_count,\n        'mean': round(mean, 6),\n        'median': round(median, 6),\n        'mode': mode_info,\n        'std_dev': round(std_dev, 6),\n        'variance': round(variance, 6),\n        'min': min(numeric_data),\n        'max': max(numeric_data),\n        'range': round(data_range, 6),\n        'q1': round(q1, 6),\n        'q3': round(q3, 6),\n        'iqr': round(iqr, 6),\n        'percentiles': {k: round(v, 6) for k, v in percentiles.items()}\n    }\n    \n    # Distribution analysis\n    result['distribution'] = {\n        'skewness': calculate_skewness(numeric_data, mean, std_dev),\n        'kurtosis': calculate_kurtosis(numeric_data, mean, std_dev),\n        'outlier_count': len(outliers),\n        'outlier_percentage': round(len(outliers) / n * 100, 2) if n > 0 else 0\n    }\n    \n    result['valid'] = True\n    return result\n\ndef calculate_median(sorted_data):\n    \"\"\"Calculate median from sorted data\"\"\"\n    n = len(sorted_{
  "problems": [
    {
      "id": 129,
      "title": "CSV File Parser and Data Extractor",
      "statement": "Parse a CSV file content and extract specific columns with data validation. Handle various delimiters (comma, semicolon, tab), quoted fields, and missing values. Return structured data with column headers and rows. For example, parse 'Name,Age,City\\nJohn,25,NYC\\nJane,30,LA' and extract specific columns with type conversion.",
      "function_signature": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):",
      "categories": ["input_output_processing", "data_parsing"],
      "python_constructs": ["csv_parsing", "string_manipulation", "data_validation"],
      "hints": [
        "Use csv module or manual parsing to handle different delimiters",
        "Handle quoted fields that may contain delimiter characters",
        "Implement type conversion for numeric columns and date parsing"
      ],
      "solution": "def parse_csv_data(csv_content, delimiter=',', extract_columns=None, convert_types=None):\n    import csv\n    from io import StringIO\n    \n    if not csv_content:\n        return {'headers': [], 'rows': [], 'error': None}\n    \n    try:\n        # Parse CSV content\n        csv_reader = csv.reader(StringIO(csv_content), delimiter=delimiter)\n        rows = list(csv_reader)\n        \n        if not rows:\n            return {'headers': [], 'rows': [], 'error': None}\n        \n        headers = rows[0]\n        data_rows = rows[1:]\n        \n        # Extract specific columns if requested\n        if extract_columns:\n            try:\n                # Find column indices\n                column_indices = []\n                for col in extract_columns:\n                    if isinstance(col, str):\n                        if col in headers:\n                            column_indices.append(headers.index(col))\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column \"{col}\" not found'}\n                    elif isinstance(col, int):\n                        if 0 <= col < len(headers):\n                            column_indices.append(col)\n                        else:\n                            return {'headers': [], 'rows': [], 'error': f'Column index {col} out of range'}\n                \n                # Extract columns\n                filtered_headers = [headers[i] for i in column_indices]\n                filtered_rows = []\n                \n                for row in data_rows:\n                    filtered_row = []\n                    for i in column_indices:\n                        if i < len(row):\n                            filtered_row.append(row[i])\n                        else:\n                            filtered_row.append('')  # Missing value\n                    filtered_rows.append(filtered_row)\n                \n                headers = filtered_headers\n                data_rows = filtered_rows\n            \n            except Exception as e:\n                return {'headers': [], 'rows': [], 'error': f'Column extraction error: {str(e)}'}\n        \n        # Apply type conversions\n        if convert_types:\n            converted_rows = []\n            for row in data_rows:\n                converted_row = []\n                for i, value in enumerate(row):\n                    if i < len(headers) and headers[i] in convert_types:\n                        try:\n                            converter = convert_types[headers[i]]\n                            if converter == 'int':\n                                converted_row.append(int(value) if value.strip() else None)\n                            elif converter == 'float':\n                                converted_row.append(float(value) if value.strip() else None)\n                            elif converter == 'bool':\n                                converted_row.append(value.lower() in ['true', '1', 'yes', 'on'])\n                            else:\n                                converted_row.append(value)\n                        except (ValueError, AttributeError):\n                            converted_row.append(None)  # Conversion failed\n                    else:\n                        converted_row.append(value)\n                converted_rows.append(converted_row)\n            data_rows = converted_rows\n        \n        return {\n            'headers': headers,\n            'rows': data_rows,\n            'row_count': len(data_rows),\n            'column_count': len(headers),\n            'error': None\n        }\n    \n    except Exception as e:\n        return {'headers': [], 'rows': [], 'error': f'Parsing error: {str(e)}'}"
      ],
      "test_cases": [
        {"input": ["Name,Age,City\\nJohn,25,NYC\\nJane,30,LA", ",", null, null], "expected": {"headers": ["Name", "Age", "City"], "rows": [["John", "25", "NYC"], ["Jane", "30", "LA"]], "row_count": 2, "column_count": 3, "error": null}},
        {"input": ["Name;Age;Salary\\nJohn;25;50000\\nJane;30;60000", ";", ["Name", "Salary"], {"Salary": "int"}], "expected": {"headers": ["Name", "Salary"], "rows": [["John", 50000], ["Jane", 60000]], "row_count": 2, "column_count": 2, "error": null}},
        {"input": ["A,B,C\\n1,2,3", ",", [0, 2], {"A": "int", "C": "int"}], "expected": {"headers": ["A", "C"], "rows": [[1, 3]], "row_count": 1, "column_count": 2, "error": null}},
        {"input": [""], "expected": {"headers": [], "rows": [], "error": null}},
        {"input": ["Name,Age\\nJohn,25", ",", ["NonExistent"], null], "expected": {"headers": [], "rows": [], "error": "Column \"NonExistent\" not found"}}
      ]
    },
    {
      "id": 130,
      "title": "File Content Word and Line Counter",
      "statement": "Count words, lines, characters, and paragraphs in text content. Provide detailed statistics including average word length, sentence count, and most frequent words. Handle different text encodings and empty lines. Return comprehensive text analysis results.",
      "function_signature": "def analyze_text_content(content, include_word_freq=True, top_words=10):",
      "categories": ["input_output_processing", "text_analysis"],
      "python_constructs": ["text_processing", "regular_expressions", "frequency_analysis"],
      "hints": [
        "Use regular expressions to identify words, sentences, and paragraphs",
        "Handle different definitions of word boundaries and punctuation",
        "Calculate frequency distribution and sort by occurrence count"
      ],
      "solution": "def analyze_text_content(content, include_word_freq=True, top_words=10):\n    import re\n    from collections import Counter\n    \n    if not isinstance(content, str):\n        return None\n    \n    if not content:\n        return {\n            'characters': 0,\n            'characters_no_spaces': 0,\n            'words': 0,\n            'lines': 0,\n            'paragraphs': 0,\n            'sentences': 0,\n            'avg_word_length': 0.0,\n            'word_frequency': {} if include_word_freq else None\n        }\n    \n    # Basic counts\n    char_count = len(content)\n    char_count_no_spaces = len(content.replace(' ', '').replace('\\t', '').replace('\\n', '').replace('\\r', ''))\n    \n    # Line count\n    lines = content.split('\\n')\n    line_count = len(lines)\n    \n    # Paragraph count (separated by empty lines)\n    paragraphs = re.split(r'\\n\\s*\\n', content.strip())\n    paragraph_count = len([p for p in paragraphs if p.strip()])\n    \n    # Word extraction and counting\n    words = re.findall(r\"\\b\\w+\\b\", content.lower())\n    word_count = len(words)\n    \n    # Average word length\n    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0.0\n    \n    # Sentence count (approximate)\n    sentences = re.split(r'[.!?]+', content)\n    sentence_count = len([s for s in sentences if s.strip()])\n    \n    result = {\n        'characters': char_count,\n        'characters_no_spaces': char_count_no_spaces,\n        'words': word_count,\n        'lines': line_count,\n        'paragraphs': paragraph_count,\n        'sentences': sentence_count,\n        'avg_word_length': round(avg_word_length, 2)\n    }\n    \n    # Word frequency analysis\n    if include_word_freq and words:\n        word_freq = Counter(words)\n        top_word_list = word_freq.most_common(top_words)\n        result['word_frequency'] = {\n            'total_unique_words': len(word_freq),\n            'top_words': top_word_list,\n            'most_common_word': top_word_list[0] if top_word_list else None\n        }\n    else:\n        result['word_frequency'] = None\n    \n    return result",
      "test_cases": [
        {"input": ["Hello world! This is a test.\\nAnother line here.", true, 5], "expected": {"characters": 46, "characters_no_spaces": 37, "words": 9, "lines": 2, "paragraphs": 1, "sentences": 2, "avg_word_length": 4.0, "word_frequency": {"total_unique_words": 9, "top_words": [["hello", 1], ["world", 1], ["this", 1], ["is", 1], ["a", 1]], "most_common_word": ["hello", 1]}}},
        {"input": [""], "expected": {"characters": 0, "characters_no_spaces": 0, "words": 0, "lines": 0, "paragraphs": 0, "sentences": 0, "avg_word_length": 0.0, "word_frequency": {}}},
        {"input": ["Word word WORD!", false, 3], "expected": {"characters": 14, "characters_no_spaces": 11, "words": 3, "lines": 1, "paragraphs": 1, "sentences": 1, "avg_word_length": 4.0, "word_frequency": null}},
        {"input": ["Line 1\\n\\nParagraph 2\\nStill paragraph 2", true, 3], "expected": {"characters": 35, "characters_no_spaces": 25, "words": 6, "lines": 4, "paragraphs": 2, "sentences": 1, "avg_word_length": 4.17, "word_frequency": {"total_unique_words": 6, "top_words": [["line", 1], ["1", 1], ["paragraph", 2]], "most_common_word": ["paragraph", 2]}}},
        {"input": ["Hello! How are you? I am fine.", true, 10], "expected": {"characters": 30, "characters_no_spaces": 24, "words": 7, "lines": 1, "paragraphs": 1, "sentences": 3, "avg_word_length": 3.0, "word_frequency": {"total_unique_words": 7, "top_words": [["hello", 1], ["how", 1], ["are", 1], ["you", 1], ["i", 1], ["am", 1], ["fine", 1]], "most_common_word": ["hello", 1]}}}
      ]
    },
    {
      "id": 131,
      "title": "JSON Data Parser and Validator",
      "statement": "Parse and validate JSON data with schema checking, nested object extraction, and data transformation. Handle malformed JSON, validate data types, and extract specific fields using JSONPath-like syntax. Return parsed data with validation results and extracted fields.",
      "function_signature": "def parse_json_data(json_string, schema=None, extract_paths=None):",
      "categories": ["input_output_processing", "data_validation"],
      "python_constructs": ["json_parsing", "data_validation", "nested_data_access"],
      "hints": [
        "Use json module for parsing and handle JSONDecodeError exceptions",
        "Implement schema validation for required fields and data types",
        "Support dot notation for nested field extraction"
      ],
      "solution": "def parse_json_data(json_string, schema=None, extract_paths=None):\n    import json\n    \n    result = {\n        'valid': False,\n        'data': None,\n        'errors': [],\n        'extracted_fields': {},\n        'schema_valid': True\n    }\n    \n    # Parse JSON\n    try:\n        data = json.loads(json_string)\n        result['valid'] = True\n        result['data'] = data\n    except json.JSONDecodeError as e:\n        result['errors'].append(f'JSON parsing error: {str(e)}')\n        return result\n    except Exception as e:\n        result['errors'].append(f'Unexpected error: {str(e)}')\n        return result\n    \n    # Schema validation\n    if schema:\n        schema_errors = validate_schema(data, schema)\n        if schema_errors:\n            result['schema_valid'] = False\n            result['errors'].extend(schema_errors)\n    \n    # Extract specific fields\n    if extract_paths:\n        for path_name, path in extract_paths.items():\n            try:\n                extracted_value = extract_nested_value(data, path)\n                result['extracted_fields'][path_name] = extracted_value\n            except Exception as e:\n                result['errors'].append(f'Extraction error for \"{path_name}\": {str(e)}')\n                result['extracted_fields'][path_name] = None\n    \n    return result\n\ndef validate_schema(data, schema):\n    errors = []\n    \n    # Check required fields\n    if 'required' in schema:\n        for field in schema['required']:\n            if field not in data:\n                errors.append(f'Missing required field: {field}')\n    \n    # Check field types\n    if 'properties' in schema:\n        for field, field_schema in schema['properties'].items():\n            if field in data:\n                expected_type = field_schema.get('type')\n                actual_value = data[field]\n                \n                if expected_type == 'string' and not isinstance(actual_value, str):\n                    errors.append(f'Field \"{field}\" should be string, got {type(actual_value).__name__}')\n                elif expected_type == 'number' and not isinstance(actual_value, (int, float)):\n                    errors.append(f'Field \"{field}\" should be number, got {type(actual_value).__name__}')\n                elif expected_type == 'integer' and not isinstance(actual_value, int):\n                    errors.append(f'Field \"{field}\" should be integer, got {type(actual_value).__name__}')\n                elif expected_type == 'boolean' and not isinstance(actual_value, bool):\n                    errors.append(f'Field \"{field}\" should be boolean, got {type(actual_value).__name__}')\n                elif expected_type == 'array' and not isinstance(actual_value, list):\n                    errors.append(f'Field \"{field}\" should be array, got {type(actual_value).__name__}')\n                elif expected_type == 'object' and not isinstance(actual_value, dict):\n                    errors.append(f'Field \"{field}\" should be object, got {type(actual_value).__name__}')\n    \n    return errors\n\ndef extract_nested_value(data, path):\n    \"\"\"Extract value using dot notation path like 'user.profile.name'\"\"\"\n    keys = path.split('.')\n    current = data\n    \n    for key in keys:\n        if isinstance(current, dict) and key in current:\n            current = current[key]\n        elif isinstance(current, list) and key.isdigit():\n            index = int(key)\n            if 0 <= index < len(current):\n                current = current[index]\n            else:\n                raise KeyError(f'List index {index} out of range')\n        else:\n            raise KeyError(f'Key \"{key}\" not found in path \"{path}\"')\n    \n    return current",
      "test_cases": [
        {"input": ["{\"name\": \"John\", \"age\": 30, \"city\": \"NYC\"}", null, null], "expected": {"valid": true, "data": {"name": "John", "age": 30, "city": "NYC"}, "errors": [], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": \"John\", \"age\": 30}", {"required": ["name", "age"], "properties": {"name": {"type": "string"}, "age": {"type": "integer"}}}, {"user_name": "name", "user_age": "age"}], "expected": {"valid": true, "data": {"name": "John", "age": 30}, "errors": [], "extracted_fields": {"user_name": "John", "user_age": 30}, "schema_valid": true}},
        {"input": ["{\"user\": {\"profile\": {\"name\": \"Alice\"}}}", null, {"nested_name": "user.profile.name"}], "expected": {"valid": true, "data": {"user": {"profile": {"name": "Alice"}}}, "errors": [], "extracted_fields": {"nested_name": "Alice"}, "schema_valid": true}},
        {"input": ["invalid json", null, null], "expected": {"valid": false, "data": null, "errors": ["JSON parsing error: Expecting value: line 1 column 1 (char 0)"], "extracted_fields": {}, "schema_valid": true}},
        {"input": ["{\"name\": 123}", {"required": ["name"], "properties": {"name": {"type": "string"}}}, null], "expected": {"valid": true, "data": {"name": 123}, "errors": ["Field \"name\" should be string, got int"], "extracted_fields": {}, "schema_valid": false}}
      ]
    },
    {
      "id": 132,
      "title": "Command Line Arguments Processor",
      "statement": "Process command line arguments with support for flags, options with values, positional arguments, and help generation. Handle short (-h) and long (--help) options, validate required arguments, and provide default values. Return parsed arguments dictionary with validation results.",
      "function_signature": "def process_command_args(args_list, arg_definitions):",
      "categories": ["input_output_processing", "argument_parsing"],
      "python_constructs": ["argument_parsing", "command_line_interface", "validation_logic"],
      "hints": [
        "Parse arguments into flags (boolean), options (key-value), and positional arguments",
        "Support both short (-h) and long (--help) argument formats",
        "Validate required arguments and apply default values"
      ],
      "solution": "def process_command_args(args_list, arg_definitions):\n    result = {\n        'parsed_args': {},\n        'positional_args': [],\n        'errors': [],\n        'help_requested': False\n    }\n    \n    if not args_list:\n        args_list = []\n    \n    # Check for help request\n    if '-h' in args_list or '--help' in args_list:\n        result['help_requested'] = True\n        result['help_text'] = generate_help_text(arg_definitions)\n        return result\n    \n    # Initialize with default values\n    for arg_name, arg_config in arg_definitions.items():\n        if 'default' in arg_config:\n            result['parsed_args'][arg_name] = arg_config['default']\n    \n    i = 0\n    while i < len(args_list):\n        arg = args_list[i]\n        \n        if arg.startswith('--'):\n            # Long option\n            option_name = arg[2:]\n            arg_config = find_arg_config(option_name, arg_definitions, 'long')\n            \n            if not arg_config:\n                result['errors'].append(f'Unknown option: {arg}')\n                i += 1\n                continue\n            \n            arg_name, config = arg_config\n            \n            if config.get('type') == 'flag':\n                result['parsed_args'][arg_name] = True\n                i += 1\n            else:\n                # Option with value\n                if i + 1 >= len(args_list):\n                    result['errors'].append(f'Option {arg} requires a value')\n                    i += 1\n                    continue\n                \n                value = args_list[i + 1]\n                converted_value = convert_arg_value(value, config.get('value_type', 'string'))\n                \n                if converted_value is None and config.get('value_type') != 'string':\n                    result['errors'].append(f'Invalid {config.get(\"value_type\")} value for {arg}: {value}')\n                else:\n                    result['parsed_args'][arg_name] = converted_value if converted_value is not None else value\n                \n                i += 2\n        \n        elif arg.startswith('-') and len(arg) > 1:\n            # Short option(s)\n            for char in arg[1:]:\n                arg_config = find_arg_config(char, arg_definitions, 'short')\n                \n                if not arg_config:\n                    result['errors'].append(f'Unknown option: -{char}')\n                    continue\n                \n                arg_name, config = arg_config\n                \n                if config.get('type') == 'flag':\n                    result['parsed_args'][arg_name] = True\n            i += 1\n        \n        else:\n            # Positional argument\n            result['positional_args'].append(arg)\n            i += 1\n    \n    # Validate required arguments\n    for arg_name, arg_config in arg_definitions.items():\n        if arg_config.get('required', False) and arg_name not in result['parsed_args']:\n            result['errors'].append(f'Required argument missing: {arg_name}')\n    \n    return result\n\ndef find_arg_config(option, arg_definitions, option_type):\n    for arg_name, config in arg_definitions.items():\n        if option_type == 'long' and config.get('long') == option:\n            return (arg_name, config)\n        elif option_type == 'short' and config.get('short') == option:\n            return (arg_name, config)\n    return None\n\ndef convert_arg_value(value, value_type):\n    try:\n        if value_type == 'int':\n            return int(value)\n        elif value_type == 'float':\n            return float(value)\n        elif value_type == 'bool':\n            return value.lower() in ['true', '1', 'yes', 'on']\n        else:\n            return value\n    except (ValueError, AttributeError):\n        return None\n\ndef generate_help_text(arg_definitions):\n    help_lines = ['Available options:']\n    \n    for arg_name, config in arg_definitions.items():\n        line_parts = []\n        \n        if config.get('short'):\n            line_parts.append(f'-{config[\"short\"]}')\n        \n        if config.get('long'):\n            line_parts.append(f'--{config[\"long\"]}')\n        \n        option_text = ', '.join(line_parts)\n        \n        if config.get('description'):\n            option_text += f': {config[\"description\"]}'\n        \n        if config.get('required'):\n            option_text += ' (required)'\n        \n        if 'default' in config:\n            option_text += f' (default: {config[\"default\"]})')\n        \n        help_lines.append(f'  {option_text}')\n    \n    return '\\n'.join(help_lines)",
      "test_cases": [
        {"input": [["--input", "file.txt", "-v", "--output", "result.txt"], {"input_file": {"long": "input", "required": true}, "verbose": {"short": "v", "type": "flag"}, "output_file": {"long": "output", "default": "output.txt"}}], "expected": {"parsed_args": {"input_file": "file.txt", "verbose": true, "output_file": "result.txt"}, "positional_args": [], "errors": [], "help_requested": false}},
        {"input": [["-h"], {"verbose": {"short": "v", "type": "flag"}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": [], "help_requested": true, "help_text": "Available options:\\n  -v: (required)"}},
        {"input": [["--count", "5", "file1", "file2"], {"count": {"long": "count", "value_type": "int", "default": 1}}], "expected": {"parsed_args": {"count": 5}, "positional_args": ["file1", "file2"], "errors": [], "help_requested": false}},
        {"input": [["--missing"], {"required_arg": {"long": "required", "required": true}}], "expected": {"parsed_args": {}, "positional_args": [], "errors": ["Unknown option: --missing", "Required argument missing: required_arg"], "help_requested": false}},
        {"input": [[], {"default_val": {"long": "default", "default": "test"}}], "expected": {"parsed_args": {"default_val": "test"}, "positional_args": [], "errors": [], "help_requested": false}}
      ]
    }
  ]
}